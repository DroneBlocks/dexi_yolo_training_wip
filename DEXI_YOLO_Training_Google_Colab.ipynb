{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-header",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DroneBlocks/dexi_yolo_training/blob/main/DEXI_YOLO_Training_Google_Colab.ipynb)\n",
    "\n",
    "# DEXI YOLO Training Tutorial - Google Colab\n",
    "\n",
    "This notebook walks you through the complete process of training a custom YOLO object detection model for drone detection using Google Colab's free GPU. We'll be working with 6 classes: bird, dog, cat, motorcycle, car, and truck.\n",
    "\n",
    "## üöÄ Quick Setup:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **T4 GPU**\n",
    "2. **Run first cell** ‚Üí Automatically clones repo and installs everything\n",
    "3. **Run all cells** ‚Üí Complete YOLO training pipeline in ~20 minutes!\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Colab Setup & Repository Clone](#colab-setup)\n",
    "2. [Environment Setup](#environment-setup)\n",
    "3. [Dataset Exploration](#dataset-exploration) \n",
    "4. [Data Augmentation](#data-augmentation)\n",
    "5. [YOLO Training](#yolo-training)\n",
    "6. [Results Analysis](#results-analysis)\n",
    "7. [Model Testing](#model-testing)\n",
    "8. [ONNX Conversion](#onnx-conversion)\n",
    "9. [Download Models](#download-models)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-setup",
   "metadata": {},
   "source": [
    "## 1. Colab Setup & Repository Clone\n",
    "\n",
    "This cell automatically detects if we're running in Google Colab and sets up everything needed:\n",
    "- Clones the DEXI YOLO Training repository\n",
    "- Installs all required dependencies\n",
    "- Pre-downloads the YOLO model\n",
    "\n",
    "**‚ö†Ô∏è Important: Enable GPU before running!**\n",
    "- Go to: **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåü GOOGLE COLAB SETUP - Clone Repository & Install Dependencies\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåü Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running locally\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository if it doesn't exist\n",
    "    if not Path('dexi_yolo_training').exists():\n",
    "        print(\"üì• Cloning DEXI YOLO Training repository...\")\n",
    "        !git clone https://github.com/DroneBlocks/dexi_yolo_training.git\n",
    "        print(\"‚úÖ Repository cloned successfully!\")\n",
    "    else:\n",
    "        print(\"üìÅ Repository already exists\")\n",
    "    \n",
    "    # Change to repo directory\n",
    "    os.chdir('dexi_yolo_training')\n",
    "    print(f\"üìÇ Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Install requirements\n",
    "    print(\"üîß Installing requirements...\")\n",
    "    !pip install -r requirements.txt -q\n",
    "    \n",
    "    # Pre-download YOLO model to speed up later steps\n",
    "    print(\"üì¶ Pre-downloading YOLO model...\")\n",
    "    from ultralytics import YOLO\n",
    "    YOLO('yolov8n.pt')  # This downloads and caches the model\n",
    "    \n",
    "    print(\"\\nüéâ Colab setup complete! Ready to train YOLO model.\")\n",
    "    print(\"üí° Make sure GPU is enabled: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "    print(\"üöÄ Expected training time with GPU: 15-25 minutes\")\n",
    "    \n",
    "else:\n",
    "    print(\"üíª Local setup detected - skipping Colab-specific steps\")\n",
    "    print(\"üìù Make sure you have activated your virtual environment\")\n",
    "    print(\"‚ö° Run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-setup",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "Let's check our hardware and import all necessary libraries. This will detect whether we have GPU acceleration available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and check hardware acceleration\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from IPython.display import Image, display\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import glob as globmodule\n",
    "\n",
    "# Set matplotlib style for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"üîß System Information:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Enhanced device detection for Colab\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üöÄ GPU: {gpu_name}\")\n",
    "    print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Colab-specific GPU info\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(f\"‚ö° Google Colab GPU acceleration enabled!\")\n",
    "        print(f\"   Expected training time: 15-25 minutes\")\n",
    "        print(f\"   Expected augmentation time: 2-3 minutes\")\n",
    "    except ImportError:\n",
    "        print(f\"üñ•Ô∏è  Local CUDA GPU detected\")\n",
    "        \n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps' \n",
    "    print(f\"‚ö° Apple Silicon MPS acceleration!\")\n",
    "    print(f\"   Expected training time: 30-45 minutes\")\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"üíª Using CPU\")\n",
    "    \n",
    "    # Colab warning if no GPU\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"‚ö†Ô∏è WARNING: GPU not enabled in Colab!\")\n",
    "        print(\"üí° Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí T4 GPU\")\n",
    "        print(\"   Then: Runtime ‚Üí Restart runtime and run all cells again\")\n",
    "    except ImportError:\n",
    "        print(f\"   Expected training time: 2-4 hours\")\n",
    "\n",
    "print(f\"\\n‚úÖ Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-exploration",
   "metadata": {},
   "source": "## 3. Dataset Exploration\n\nLet's explore our dataset structure and examine the original 6 images that will be used for augmentation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-dataset",
   "metadata": {},
   "outputs": [],
   "source": "# Find and display our original images\noriginal_images_path = Path('train/original_image')\nimage_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n\n# Find all image files\nall_images = []\nfor ext in image_extensions:\n    all_images.extend(original_images_path.glob(ext))\n    all_images.extend(original_images_path.glob(ext.upper()))\n\nprint(f\"üì∏ Found {len(all_images)} original images in the dataset\")\nprint(f\"üí° These will be augmented to create 900 training images (150 per class)\")\n\n# Display original images\nif all_images:\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    fig.suptitle('Original Images for YOLO Training', fontsize=16, fontweight='bold')\n    \n    for idx, img_path in enumerate(all_images[:6]):\n        if idx >= 6:\n            break\n        \n        row = idx // 3\n        col = idx % 3\n        \n        # Load and display image\n        img = cv2.imread(str(img_path))\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        axes[row, col].imshow(img_rgb)\n        axes[row, col].set_title(f\"{img_path.stem.upper()}\\n{img.shape[1]}x{img.shape[0]}px\", fontweight='bold')\n        axes[row, col].axis('off')\n    \n    # Hide unused subplots\n    for idx in range(len(all_images), 6):\n        row = idx // 3\n        col = idx % 3\n        axes[row, col].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nüìä Original Images Summary:\")\n    for img_path in all_images[:6]:\n        img = cv2.imread(str(img_path))\n        size_mb = img_path.stat().st_size / (1024*1024)\n        print(f\"  {img_path.name}: {img.shape[1]}x{img.shape[0]} ({size_mb:.1f} MB)\")\n        \nelse:\n    print(\"‚ùå No images found in the train/original_image directory\")\n    print(\"üîç Make sure the repository was cloned correctly\")"
  },
  {
   "cell_type": "markdown",
   "id": "data-augmentation",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation\n",
    "\n",
    "Now we'll use our custom augmentation script to create 150 variations of each base image (900 total). This is crucial for training a robust YOLO model as it helps the model generalize better to different conditions.\n",
    "\n",
    "**‚è±Ô∏è Expected time: 2-3 minutes with GPU, 5-7 minutes with CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation-overview",
   "metadata": {},
   "outputs": [],
   "source": "# Let's examine our augmentation script first\nprint(\"üîß Data Augmentation Pipeline Overview:\")\nprint(\"\")\nprint(\"Our augmentation script applies the following transformations:\")\nprint(\"‚Ä¢ üîÑ Rotation: 0-360 degrees (random)\")\nprint(\"‚Ä¢ üìè Scaling: 0.25x to 1.3x (random)\")\nprint(\"‚Ä¢ ‚òÄÔ∏è Brightness: -30 to +30 (random)\")\nprint(\"‚Ä¢ üåà Contrast: 0.7x to 1.3x (random)\")\nprint(\"‚Ä¢ üìª Noise: Added 20% of the time\")\nprint(\"‚Ä¢ üå´Ô∏è Blur: Applied 15% of the time\")\nprint(\"\")\nprint(\"Each transformation creates realistic variations that help the model\")\nprint(\"learn to detect objects under different lighting, weather, and camera conditions.\")\nprint(\"\")\nprint(\"üí° This technique dramatically improves model robustness with minimal data!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation-config",
   "metadata": {},
   "outputs": [],
   "source": "# Set augmentation parameters\nAUGMENTATIONS_PER_IMAGE = 150  # Creates 900 total images (6 classes √ó 150)\nINPUT_DIR = \"train/original_image\"     # Directory with original images\nOUTPUT_DIR = \"train\"          # Output directory for augmented dataset\n\nprint(f\"‚öôÔ∏è Augmentation Configuration:\")\nprint(f\"   Input directory: {INPUT_DIR}\")\nprint(f\"   Output directory: {OUTPUT_DIR}\")\nprint(f\"   Augmentations per image: {AUGMENTATIONS_PER_IMAGE}\")\nprint(f\"   Expected total images: {len(all_images) * AUGMENTATIONS_PER_IMAGE}\")\nprint(f\"\")\nprint(f\"üìä Training Dataset Breakdown:\")\nfor img_path in all_images[:6]:\n    class_name = img_path.stem\n    print(f\"   {class_name}: {AUGMENTATIONS_PER_IMAGE} augmented images\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the augmentation process\n",
    "print(\"üöÄ Starting data augmentation...\")\n",
    "\n",
    "# Time estimation based on environment\n",
    "if device == 'cuda':\n",
    "    print(\"‚ö° GPU acceleration detected - estimated time: 2-3 minutes\")\n",
    "elif device == 'mps':\n",
    "    print(\"üçé Apple Silicon detected - estimated time: 3-4 minutes\")\n",
    "else:\n",
    "    print(\"üíª CPU processing - estimated time: 5-7 minutes\")\n",
    "\n",
    "print(\"üìà Progress will be shown below...\")\n",
    "print(\"\")\n",
    "\n",
    "# Import and use our augmentation class\n",
    "from augment_dataset import YOLODatasetAugmenter\n",
    "\n",
    "# Create augmenter instance\n",
    "augmenter = YOLODatasetAugmenter(INPUT_DIR, OUTPUT_DIR)\n",
    "\n",
    "# Run augmentation with progress tracking\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "augmenter.augment_all_images(AUGMENTATIONS_PER_IMAGE)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Data augmentation completed in {duration/60:.1f} minutes!\")\n",
    "print(f\"üéØ Ready for YOLO training with {len(all_images) * AUGMENTATIONS_PER_IMAGE} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify augmentation results\n",
    "train_images_dir = Path('train/images')\n",
    "train_labels_dir = Path('train/labels')\n",
    "\n",
    "# Count generated files\n",
    "augmented_images = list(train_images_dir.glob('*_[0-9][0-9][0-9].jpg'))\n",
    "augmented_labels = list(train_labels_dir.glob('*_[0-9][0-9][0-9].txt'))\n",
    "original_images = [f for f in train_images_dir.glob('*.jpg') if not f.name.endswith(('_001.jpg', '_002.jpg', '_003.jpg'))]\n",
    "\n",
    "print(f\"üìä Augmentation Results Summary:\")\n",
    "print(f\"   Original base images: {len(original_images)}\")\n",
    "print(f\"   Generated images: {len(augmented_images)}\")\n",
    "print(f\"   Total training images: {len(list(train_images_dir.glob('*.jpg')))}\")\n",
    "print(f\"   Label files created: {len(augmented_labels)}\")\n",
    "print(f\"\")\n",
    "print(f\"üéØ Images per class (including originals):\")\n",
    "for class_name in ['bird', 'dog', 'cat', 'motorcycle', 'car', 'truck']:\n",
    "    class_images = len(list(train_images_dir.glob(f'{class_name}*.jpg')))\n",
    "    print(f\"   {class_name.title()}: {class_images} images\")\n",
    "\n",
    "# Verify dataset balance\n",
    "total_images = len(list(train_images_dir.glob('*.jpg')))\n",
    "if total_images == 906:  # 6 original + 900 augmented\n",
    "    print(f\"\\n‚úÖ Dataset verification passed! Ready for training.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Expected 906 images, found {total_images}. Check augmentation process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-augmented-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples of augmented images\n",
    "print(\"üñºÔ∏è Sample Augmented Images (Showing Transformations):\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Augmented Training Images - Variety of Transformations', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show examples from each class\n",
    "classes = ['bird', 'dog', 'cat', 'motorcycle', 'car', 'truck']\n",
    "sample_count = 0\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    # Get first 2 augmented images of each class\n",
    "    class_images = list(train_images_dir.glob(f'{class_name}_*.jpg'))[:2]\n",
    "    \n",
    "    for img_idx, img_path in enumerate(class_images):\n",
    "        if sample_count >= 12:  # 3x4 grid\n",
    "            break\n",
    "            \n",
    "        row = sample_count // 4\n",
    "        col = sample_count % 4\n",
    "        \n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[row, col].imshow(img_rgb)\n",
    "        axes[row, col].set_title(f\"{class_name.title()}\\n{img_path.name}\", fontweight='bold', fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        sample_count += 1\n",
    "    \n",
    "    if sample_count >= 12:\n",
    "        break\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(sample_count, 12):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Notice the variety in rotations, brightness, and transformations!\")\n",
    "print(\"üß† This diversity helps the model generalize to real-world conditions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yolo-training",
   "metadata": {},
   "source": [
    "## 5. YOLO Training\n",
    "\n",
    "Now that we have our augmented dataset ready, let's train our YOLO model! We'll use the YOLOv8 nano architecture, which is perfect for edge deployment while still achieving excellent accuracy.\n",
    "\n",
    "**‚è±Ô∏è Expected time: 15-25 minutes with T4 GPU, 30-45 minutes with local GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration optimized for Colab\n",
    "TRAINING_CONFIG = {\n",
    "    'model_size': 'n',        # Nano: Fast, efficient, perfect for Pi deployment\n",
    "    'epochs': 100,            # Number of training epochs\n",
    "    'imgsz': 640,             # Image size for training\n",
    "    'batch_size': 16 if device == 'cuda' else 8,  # Adjust for GPU memory\n",
    "    'device': device,         # Device determined earlier\n",
    "}\n",
    "\n",
    "print(\"üéØ Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(\"\\nüí° YOLOv8 Model Size Guide:\")\n",
    "print(\"   ‚Ä¢ 'n' (nano): ‚ö° Fastest, smallest, perfect for Pi/mobile deployment\")\n",
    "print(\"   ‚Ä¢ 's' (small): üîÑ Good balance of speed and accuracy\")\n",
    "print(\"   ‚Ä¢ 'm' (medium): üéØ Better accuracy, moderate speed\")\n",
    "print(\"   ‚Ä¢ 'l' (large): üèÜ High accuracy, slower inference\")\n",
    "print(\"   ‚Ä¢ 'x' (extra-large): ü•á Highest accuracy, slowest inference\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance Expectations ({device.upper()}):\")\n",
    "if device == 'cuda':\n",
    "    print(\"   Training time: ~15-25 minutes\")\n",
    "    print(\"   Expected mAP@0.5: 99%+ (excellent!)\")\n",
    "elif device == 'mps':\n",
    "    print(\"   Training time: ~30-45 minutes\") \n",
    "    print(\"   Expected mAP@0.5: 99%+ (excellent!)\")\n",
    "else:\n",
    "    print(\"   Training time: 2-4 hours\")\n",
    "    print(\"   Consider enabling GPU for much faster training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the YOLO model\n",
    "model_name = f\"yolov8{TRAINING_CONFIG['model_size']}.pt\"\n",
    "print(f\"ü§ñ Loading {model_name} model...\")\n",
    "\n",
    "# Load pre-trained YOLO model\n",
    "model = YOLO(model_name)\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Model: YOLOv8{TRAINING_CONFIG['model_size']}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.model.parameters()):,}\")\n",
    "print(f\"   Model size: {os.path.getsize(model_name) / (1024*1024):.1f} MB\")\n",
    "print(f\"   Perfect for Raspberry Pi deployment! ü•ß\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": "# Start training with progress monitoring\nprint(\"üöÄ Starting YOLO training...\")\nprint(\"üìä You'll see training progress, loss curves, and validation metrics below.\")\nprint(\"üíæ Training logs and checkpoints will be saved in 'runs/detect/drone_detection/'\")\nprint(\"üí° YOLO will automatically split training data for validation (80% train, 20% val)\")\nprint(\"\")\n\n# Time estimation\nif device == 'cuda':\n    print(\"‚è±Ô∏è  Estimated completion: 15-25 minutes\")\n    print(\"‚òï Perfect time for a coffee break!\")\nelif device == 'mps':\n    print(\"‚è±Ô∏è  Estimated completion: 30-45 minutes\")\n    print(\"üçµ Great time for tea and checking emails!\")\nelse:\n    print(\"‚è±Ô∏è  Estimated completion: 2-4 hours\")\n    print(\"üí° Consider enabling GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n\nprint(\"\\nüéØ Training starting...\")\nprint(\"=\" * 60)\n\nimport time\ntraining_start_time = time.time()\n\n# Train the model with optimized parameters\nresults = model.train(\n    data='dataset.yaml',\n    epochs=TRAINING_CONFIG['epochs'],\n    imgsz=TRAINING_CONFIG['imgsz'],\n    batch=TRAINING_CONFIG['batch_size'],\n    device=TRAINING_CONFIG['device'],\n    project='runs/detect',\n    name='drone_detection',\n    save_period=20,      # Save checkpoint every 20 epochs (less frequent for Colab)\n    patience=25,         # Early stopping patience\n    \n    # Automatic train/validation split (no separate val folder needed)\n    fraction=0.8,        # 80% for training, 20% for validation\n    \n    # Augmentation settings (additional to our pre-generated augmentations)\n    hsv_h=0.015,         # Hue augmentation\n    hsv_s=0.7,           # Saturation augmentation  \n    hsv_v=0.4,           # Value augmentation\n    degrees=0,           # No additional rotation (we already did this)\n    translate=0.1,       # Translation augmentation\n    scale=0.1,           # Additional scale augmentation\n    shear=0.1,           # Shear augmentation\n    perspective=0.0,     # Perspective augmentation\n    flipud=0.0,          # No vertical flip (objects have orientation)\n    fliplr=0.0,          # No horizontal flip (for consistency)\n    mosaic=0.8,          # Mosaic augmentation probability\n    mixup=0.1,           # Mixup augmentation probability\n    \n    # Optimization settings\n    optimizer='AdamW',   # Adam with weight decay\n    lr0=0.01,            # Initial learning rate\n    lrf=0.1,             # Final learning rate (lr0 * lrf)\n    momentum=0.937,      # SGD momentum\n    weight_decay=0.0005, # Weight decay for regularization\n    warmup_epochs=3,     # Warm-up epochs\n    warmup_momentum=0.8, # Warm-up momentum\n    warmup_bias_lr=0.1,  # Warm-up bias learning rate\n    \n    # Loss function weights\n    box=7.5,             # Box loss gain\n    cls=0.5,             # Class loss gain  \n    dfl=1.5,             # DFL loss gain\n    verbose=True,        # Verbose output\n)\n\ntraining_end_time = time.time()\ntraining_duration = training_end_time - training_start_time\n\nprint(\"=\" * 60)\nprint(f\"üéâ Training completed in {training_duration/60:.1f} minutes!\")\nprint(f\"üèÜ Model ready for deployment!\")"
  },
  {
   "cell_type": "markdown",
   "id": "results-analysis",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "Let's analyze the training results and visualize the model's performance with charts and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest training results directory\n",
    "results_dirs = globmodule.glob('runs/detect/drone_detection*')\n",
    "if results_dirs:\n",
    "    results_dir = Path(max(results_dirs, key=os.path.getmtime))\n",
    "    print(f\"üìÇ Latest training results: {results_dir}\")\n",
    "    print(f\"üìÅ Training artifacts:\")\n",
    "    for item in sorted(results_dir.iterdir()):\n",
    "        if item.is_file():\n",
    "            print(f\"   üìÑ {item.name}\")\n",
    "        else:\n",
    "            print(f\"   üìÅ {item.name}/\")\n",
    "else:\n",
    "    print(\"‚ùå Training results not found. Make sure training completed successfully.\")\n",
    "    results_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-training-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training curves\n",
    "if results_dir and results_dir.exists():\n",
    "    results_image_path = results_dir / 'results.png'\n",
    "    if results_image_path.exists():\n",
    "        print(\"üìà Training Results & Performance Curves:\")\n",
    "        print(\"These show how the model learned over time - loss should decrease, mAP should increase!\")\n",
    "        display(Image(str(results_image_path)))\n",
    "        \n",
    "        print(\"\\nüìä How to read these charts:\")\n",
    "        print(\"‚Ä¢ üìâ Loss curves (lower is better): Shows training progress\")\n",
    "        print(\"‚Ä¢ üìà mAP curves (higher is better): Shows detection accuracy\")\n",
    "        print(\"‚Ä¢ üéØ Precision/Recall: Shows detection quality\")\n",
    "        print(\"‚Ä¢ ‚ö° Look for: Decreasing loss, increasing mAP > 0.9\")\n",
    "    else:\n",
    "        print(\"üìà Training curves not found. They should be available after training completes.\")\n",
    "else:\n",
    "    print(\"‚ùå No results directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "if results_dir and results_dir.exists():\n",
    "    confusion_matrix_path = results_dir / 'confusion_matrix.png'\n",
    "    if confusion_matrix_path.exists():\n",
    "        print(\"üéØ Confusion Matrix - Model Classification Accuracy:\")\n",
    "        print(\"This shows how well the model distinguishes between different classes.\")\n",
    "        print(\"Perfect diagonal = 100% accuracy for each class!\")\n",
    "        display(Image(str(confusion_matrix_path)))\n",
    "        \n",
    "        print(\"\\nüìä How to read the confusion matrix:\")\n",
    "        print(\"‚Ä¢ Diagonal (top-left to bottom-right): Correct predictions\")\n",
    "        print(\"‚Ä¢ Off-diagonal: Misclassifications between classes\")\n",
    "        print(\"‚Ä¢ Darker blue = higher values = better performance\")\n",
    "        print(\"‚Ä¢ Goal: Dark diagonal, light off-diagonal areas\")\n",
    "    else:\n",
    "        print(\"üéØ Confusion matrix not found.\")\n",
    "else:\n",
    "    print(\"‚ùå No results directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display validation batch results\n",
    "if results_dir and results_dir.exists():\n",
    "    val_batch_path = results_dir / 'val_batch0_labels.jpg'\n",
    "    val_pred_path = results_dir / 'val_batch0_pred.jpg'\n",
    "    \n",
    "    if val_batch_path.exists():\n",
    "        print(\"üîç Validation Batch - Ground Truth Labels:\")\n",
    "        print(\"These are the 'correct answers' the model should learn.\")\n",
    "        display(Image(str(val_batch_path)))\n",
    "    \n",
    "    if val_pred_path.exists():\n",
    "        print(\"\\nü§ñ Validation Batch - Model Predictions:\")\n",
    "        print(\"These are what the model actually predicted. Compare with ground truth above!\")\n",
    "        print(\"Good predictions = boxes in same locations with correct labels and high confidence.\")\n",
    "        display(Image(str(val_pred_path)))\n",
    "        \n",
    "        if val_batch_path.exists():\n",
    "            print(\"\\nüí° Comparison Tips:\")\n",
    "            print(\"‚Ä¢ ‚úÖ Bounding boxes should align well between ground truth and predictions\")\n",
    "            print(\"‚Ä¢ ‚úÖ Class labels should match (bird, car, dog, etc.)\")\n",
    "            print(\"‚Ä¢ ‚úÖ Confidence scores should be high (>0.5, ideally >0.8)\")\n",
    "            print(\"‚Ä¢ üéØ Perfect alignment = excellent model performance!\")\n",
    "\n",
    "    if not val_batch_path.exists() and not val_pred_path.exists():\n",
    "        print(\"üîç Validation images not found.\")\n",
    "else:\n",
    "    print(\"‚ùå No results directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and run final validation\n",
    "if results_dir and results_dir.exists():\n",
    "    best_model_path = results_dir / 'weights' / 'best.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"üèÜ Loading best trained model: {best_model_path}\")\n",
    "        trained_model = YOLO(str(best_model_path))\n",
    "        \n",
    "        print(f\"üìä Model file size: {best_model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"ü•ß Perfect size for Raspberry Pi deployment!\")\n",
    "        \n",
    "        # Run final validation\n",
    "        print(\"\\nüî¨ Running final validation...\")\n",
    "        val_results = trained_model.val()\n",
    "        \n",
    "        # Print key metrics with explanations\n",
    "        print(\"\\nüéØ Final Model Performance Metrics:\")\n",
    "        print(\"=\" * 50)\n",
    "        if hasattr(val_results, 'box'):\n",
    "            metrics = val_results.box\n",
    "            map50 = metrics.map50\n",
    "            map_5095 = metrics.map\n",
    "            precision = metrics.mp\n",
    "            recall = metrics.mr\n",
    "            \n",
    "            print(f\"üìà mAP@0.5:      {map50:.3f} ({'ü•á Excellent!' if map50 > 0.95 else 'ü•à Very Good!' if map50 > 0.90 else 'ü•â Good!' if map50 > 0.80 else '‚ö†Ô∏è Needs improvement'})\")\n",
    "            print(f\"üìà mAP@0.5:0.95: {map_5095:.3f} ({'ü•á Excellent!' if map_5095 > 0.90 else 'ü•à Very Good!' if map_5095 > 0.80 else 'ü•â Good!' if map_5095 > 0.70 else '‚ö†Ô∏è Needs improvement'})\")\n",
    "            print(f\"üéØ Precision:    {precision:.3f} (How accurate are detections?)\")\n",
    "            print(f\"üîç Recall:       {recall:.3f} (How many objects were found?)\")\n",
    "            \n",
    "            print(\"\\nüí° Metric Explanations:\")\n",
    "            print(\"‚Ä¢ mAP@0.5: Average precision at 50% overlap threshold\")\n",
    "            print(\"‚Ä¢ mAP@0.5:0.95: Average precision across multiple thresholds (more strict)\")\n",
    "            print(\"‚Ä¢ Precision: Of all detections, how many were correct?\")\n",
    "            print(\"‚Ä¢ Recall: Of all ground truth objects, how many were detected?\")\n",
    "            \n",
    "            # Performance assessment\n",
    "            if map50 > 0.95 and map_5095 > 0.90:\n",
    "                print(\"\\nüéâ OUTSTANDING PERFORMANCE! This model is ready for production deployment.\")\n",
    "            elif map50 > 0.90 and map_5095 > 0.80:\n",
    "                print(\"\\n‚úÖ EXCELLENT PERFORMANCE! This model will work very well in real-world scenarios.\")\n",
    "            elif map50 > 0.80 and map_5095 > 0.70:\n",
    "                print(\"\\nüëç GOOD PERFORMANCE! This model should work well for most use cases.\")\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è Model performance could be improved. Consider more training data or different parameters.\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Could not extract validation metrics.\")\n",
    "    else:\n",
    "        print(\"‚ùå Best model not found. Training may not have completed successfully.\")\n",
    "        trained_model = None\n",
    "        best_model_path = None\n",
    "else:\n",
    "    print(\"‚ùå No results directory found.\")\n",
    "    trained_model = None\n",
    "    best_model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-testing",
   "metadata": {},
   "source": [
    "## 7. Model Testing\n",
    "\n",
    "Let's test our trained model on some sample images to see how well it performs in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on sample training images\n",
    "if 'trained_model' in locals() and trained_model is not None:\n",
    "    print(\"üß™ Testing the trained model on sample images...\")\n",
    "    print(\"This shows how well the model detects and classifies objects!\")\n",
    "    \n",
    "    # Get some test images (first augmented image of each class)\n",
    "    train_images_dir = Path('train/images')\n",
    "    test_images = list(train_images_dir.glob('*_001.jpg'))[:6]\n",
    "    \n",
    "    if test_images:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('üîç Model Predictions on Test Images', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for idx, img_path in enumerate(test_images[:6]):\n",
    "            row = idx // 3\n",
    "            col = idx % 3\n",
    "            \n",
    "            # Run inference\n",
    "            results = trained_model(str(img_path), verbose=False, conf=0.5)\n",
    "            \n",
    "            # Get the annotated image with predictions\n",
    "            annotated_img = results[0].plot()\n",
    "            annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[row, col].imshow(annotated_img_rgb)\n",
    "            axes[row, col].set_title(f\"Test: {img_path.stem}\", fontweight='bold')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüéØ What to look for in the predictions:\")\n",
    "        print(\"‚Ä¢ ‚úÖ Bounding boxes around objects (colored rectangles)\")\n",
    "        print(\"‚Ä¢ üè∑Ô∏è  Class labels (bird, car, dog, cat, motorcycle, truck)\")\n",
    "        print(\"‚Ä¢ üìä Confidence scores (higher = more confident, >0.5 is good)\")\n",
    "        print(\"‚Ä¢ üé® Different colors for different classes\")\n",
    "        print(\"\\nüí° High confidence + correct labels = excellent model performance!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No test images found.\")\n",
    "else:\n",
    "    print(\"‚ùå Trained model not available. Please complete the training step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-test-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test model on any image (for future use)\n",
    "def test_on_custom_image(image_path, confidence_threshold=0.5):\n",
    "    \"\"\"Test the trained model on a custom image\"\"\"\n",
    "    if 'trained_model' not in locals() or trained_model is None:\n",
    "        print(\"‚ùå Trained model not available. Please complete the training step first.\")\n",
    "        return\n",
    "    \n",
    "    if not Path(image_path).exists():\n",
    "        print(f\"‚ùå Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Testing model on: {image_path}\")\n",
    "    \n",
    "    # Run inference\n",
    "    results = trained_model(image_path, conf=confidence_threshold, verbose=False)\n",
    "    \n",
    "    # Display results\n",
    "    annotated_img = results[0].plot()\n",
    "    annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(annotated_img_rgb)\n",
    "    plt.title(f'üéØ Detection Results - {Path(image_path).name}', fontweight='bold', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load dataset config to get class names\n",
    "    with open('dataset.yaml', 'r') as f:\n",
    "        dataset_config = yaml.safe_load(f)\n",
    "    \n",
    "    # Print detection details\n",
    "    if len(results[0].boxes) > 0:\n",
    "        print(\"\\nüéØ Detections found:\")\n",
    "        for i, box in enumerate(results[0].boxes):\n",
    "            class_id = int(box.cls[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            class_name = dataset_config['names'][class_id]\n",
    "            print(f\"   {i+1}. {class_name.upper()} (confidence: {confidence:.3f})\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No objects detected above confidence threshold\")\n",
    "        print(f\"üí° Try lowering confidence_threshold (current: {confidence_threshold})\")\n",
    "\n",
    "print(\"üìù Custom testing function created!\")\n",
    "print(\"üí° Usage example:\")\n",
    "print(\"   test_on_custom_image('path/to/your/image.jpg', confidence_threshold=0.3)\")\n",
    "print(\"   (Upload your own images to test the model!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "onnx-conversion",
   "metadata": {},
   "source": [
    "## 8. ONNX Conversion for Deployment\n",
    "\n",
    "Let's convert our trained PyTorch model to ONNX format for efficient deployment on devices like Raspberry Pi. ONNX models are faster and more compatible across different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onnx-conversion-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to ONNX format optimized for Pi deployment\n",
    "if 'best_model_path' in locals() and best_model_path and best_model_path.exists():\n",
    "    print(\"üöÄ Converting trained PyTorch model to ONNX format...\")\n",
    "    print(\"ü•ß Optimizing for Raspberry Pi deployment!\")\n",
    "    \n",
    "    try:\n",
    "        # Use built-in YOLO export function (simpler and more reliable than custom script)\n",
    "        print(\"\\n‚öôÔ∏è Export Configuration:\")\n",
    "        print(\"   Format: ONNX\")\n",
    "        print(\"   Image size: 320x320 (optimized for Pi camera)\")\n",
    "        print(\"   Precision: FP32 (better Pi compatibility)\")\n",
    "        print(\"   Simplification: Enabled (faster inference)\")\n",
    "        \n",
    "        # Export the model\n",
    "        onnx_path = trained_model.export(\n",
    "            format='onnx',\n",
    "            imgsz=320,        # Smaller size for Pi camera (320x240)\n",
    "            half=False,       # Use FP32 for better Pi compatibility\n",
    "            simplify=True,    # Simplify the ONNX graph for better performance\n",
    "            opset=11,         # ONNX opset version (compatible with most systems)\n",
    "        )\n",
    "        \n",
    "        if onnx_path and Path(onnx_path).exists():\n",
    "            print(f\"\\n‚úÖ ONNX conversion successful!\")\n",
    "            print(f\"üìÅ ONNX model saved: {onnx_path}\")\n",
    "            \n",
    "            # Show file sizes\n",
    "            pytorch_size = best_model_path.stat().st_size / (1024*1024)\n",
    "            onnx_size = Path(onnx_path).stat().st_size / (1024*1024)\n",
    "            print(f\"üìä File size comparison:\")\n",
    "            print(f\"   PyTorch (.pt): {pytorch_size:.1f} MB\")\n",
    "            print(f\"   ONNX (.onnx):  {onnx_size:.1f} MB\")\n",
    "            \n",
    "            # Test the ONNX model\n",
    "            try:\n",
    "                import onnxruntime as ort\n",
    "                session = ort.InferenceSession(onnx_path)\n",
    "                inputs = session.get_inputs()\n",
    "                outputs = session.get_outputs()\n",
    "                \n",
    "                print(f\"\\nüîç ONNX Model Verification:\")\n",
    "                print(f\"   ‚úÖ Model loads successfully\")\n",
    "                print(f\"   üìê Input shape: {inputs[0].shape}\")\n",
    "                print(f\"   üìä Output shape: {outputs[0].shape}\")\n",
    "                print(f\"   üñ•Ô∏è  Available providers: {ort.get_available_providers()}\")\n",
    "                \n",
    "                print(f\"\\nüéâ Model ready for Raspberry Pi deployment!\")\n",
    "                \n",
    "            except ImportError:\n",
    "                print(f\"\\n‚ö†Ô∏è  ONNX Runtime not available for verification\")\n",
    "                print(f\"   Model conversion completed, but couldn't verify\")\n",
    "                print(f\"   Install with: pip install onnxruntime\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå ONNX conversion failed - file not created\")\n",
    "            onnx_path = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ONNX conversion failed: {e}\")\n",
    "        print(f\"üí° The PyTorch model still works perfectly for most deployments\")\n",
    "        onnx_path = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No trained model found. Please complete training first.\")\n",
    "    onnx_path = None\n",
    "\n",
    "# Deployment instructions\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ü•ß RASPBERRY PI DEPLOYMENT INSTRUCTIONS\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"\"\"üìã Quick setup steps:\n",
    "\n",
    "1. üì• Download your model files (next cell)\n",
    "2. üì§ Copy to your Raspberry Pi:\n",
    "   scp best.pt pi@your-pi-ip:~/\n",
    "   \n",
    "3. üîß Install dependencies on Pi:\n",
    "   pip install ultralytics opencv-python\n",
    "   \n",
    "4. üêç Python inference code:\n",
    "   ```python\n",
    "   from ultralytics import YOLO\n",
    "   model = YOLO('best.pt')\n",
    "   results = model('image.jpg')\n",
    "   results[0].show()\n",
    "   ```\n",
    "\n",
    "5. üìπ For live camera detection:\n",
    "   Use the pi_camera_inference.py script included in the repo!\n",
    "   \n",
    "üéØ Classes: bird, dog, cat, motorcycle, car, truck\n",
    "‚ö° Expected Pi performance: 2-5 FPS (great for real-time detection!)\n",
    "\"\"\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-models",
   "metadata": {},
   "source": [
    "## 9. Download Your Trained Models\n",
    "\n",
    "üéâ **Congratulations!** Your YOLO model is trained and ready for deployment. Let's download the model files so you can use them on your Raspberry Pi or other devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-models-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained models (Colab only)\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"üì• Preparing model downloads for local use...\")\n",
    "    print(\"üéØ These files are ready for Raspberry Pi deployment!\")\n",
    "    \n",
    "    download_count = 0\n",
    "    \n",
    "    # Download the best PyTorch model\n",
    "    if 'best_model_path' in locals() and best_model_path and best_model_path.exists():\n",
    "        print(f\"\\nüì¶ Downloading: {best_model_path.name}\")\n",
    "        print(f\"   Size: {best_model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"   Use: Primary model for inference\")\n",
    "        files.download(str(best_model_path))\n",
    "        download_count += 1\n",
    "        \n",
    "        # Also download the last model (in case best model has issues)\n",
    "        last_model_path = best_model_path.parent / 'last.pt'\n",
    "        if last_model_path.exists():\n",
    "            print(f\"\\nüì¶ Downloading: {last_model_path.name} (backup)\")\n",
    "            print(f\"   Size: {last_model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "            print(f\"   Use: Alternative if best.pt has issues\")\n",
    "            files.download(str(last_model_path))\n",
    "            download_count += 1\n",
    "    \n",
    "    # Download ONNX model if it exists\n",
    "    if 'onnx_path' in locals() and onnx_path and Path(onnx_path).exists():\n",
    "        print(f\"\\nüì¶ Downloading: {Path(onnx_path).name}\")\n",
    "        print(f\"   Size: {Path(onnx_path).stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"   Use: Optimized for faster inference\")\n",
    "        files.download(str(onnx_path))\n",
    "        download_count += 1\n",
    "    \n",
    "    # Download dataset config\n",
    "    dataset_config_path = Path('dataset.yaml')\n",
    "    if dataset_config_path.exists():\n",
    "        print(f\"\\nüì¶ Downloading: {dataset_config_path.name}\")\n",
    "        print(f\"   Use: Contains class names and dataset info\")\n",
    "        files.download(str(dataset_config_path))\n",
    "        download_count += 1\n",
    "    \n",
    "    # Download Pi inference script\n",
    "    pi_script_path = Path('pi_camera_inference.py')\n",
    "    if pi_script_path.exists():\n",
    "        print(f\"\\nüì¶ Downloading: {pi_script_path.name}\")\n",
    "        print(f\"   Use: Ready-to-run Pi camera detection script\")\n",
    "        files.download(str(pi_script_path))\n",
    "        download_count += 1\n",
    "    \n",
    "    if download_count > 0:\n",
    "        print(f\"\\n‚úÖ Downloaded {download_count} files to your browser's Downloads folder!\")\n",
    "        print(f\"üöÄ Your YOLO model is ready for deployment!\")\n",
    "        \n",
    "        print(f\"\\nüéØ Quick Start on Raspberry Pi:\")\n",
    "        print(f\"1. Copy best.pt to your Pi\")\n",
    "        print(f\"2. Install: pip install ultralytics opencv-python\")\n",
    "        print(f\"3. Run: python pi_camera_inference.py\")\n",
    "        print(f\"4. Point camera at objects and watch the magic! ü™Ñ\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No model files found to download.\")\n",
    "        print(\"üîç Make sure training completed successfully.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"üíª Local environment detected - no downloads needed\")\n",
    "    print(\"üóÇÔ∏è  Your trained models are saved in:\")\n",
    "    if 'results_dir' in locals() and results_dir:\n",
    "        print(f\"   üìÅ {results_dir}/weights/\")\n",
    "        print(f\"   üìÑ best.pt (main model)\")\n",
    "        print(f\"   üìÑ last.pt (backup model)\")\n",
    "        if 'onnx_path' in locals() and onnx_path:\n",
    "            print(f\"   üìÑ {Path(onnx_path).name} (ONNX version)\")\n",
    "    else:\n",
    "        print(\"   üîç Check runs/detect/drone_detection*/weights/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## üéâ Congratulations! Mission Accomplished!\n",
    "\n",
    "You've successfully completed the **DEXI YOLO Training Pipeline** in Google Colab! üöÄ\n",
    "\n",
    "### ‚úÖ What You've Accomplished:\n",
    "\n",
    "1. **üåü Environment Setup**: Automatically cloned repo and configured Colab with GPU acceleration\n",
    "2. **üìä Dataset Exploration**: Examined your 6 base images for training\n",
    "3. **üîÑ Data Augmentation**: Generated 900 diverse training images from just 6 originals\n",
    "4. **ü§ñ YOLO Training**: Trained a custom YOLOv8 nano model with excellent accuracy\n",
    "5. **üìà Results Analysis**: Visualized training curves, confusion matrix, and validation results\n",
    "6. **üß™ Model Testing**: Verified predictions on test images\n",
    "7. **‚ö° ONNX Conversion**: Optimized model for fast Raspberry Pi deployment\n",
    "8. **üì• Model Download**: Downloaded all files needed for deployment\n",
    "\n",
    "### üèÜ Your Model Performance:\n",
    "- **üéØ Detection Classes**: Bird, Dog, Cat, Motorcycle, Car, Truck\n",
    "- **üìä Expected Accuracy**: 99%+ mAP@0.5 (exceptional!)\n",
    "- **‚ö° Model Size**: ~6MB (perfect for edge devices)\n",
    "- **ü•ß Pi Performance**: 2-5 FPS real-time detection\n",
    "\n",
    "### üöÄ Next Steps - Deploy Your Model:\n",
    "\n",
    "#### **Raspberry Pi Deployment:**\n",
    "```bash\n",
    "# 1. Copy your model to Pi\n",
    "scp best.pt pi@your-pi-ip:~/\n",
    "\n",
    "# 2. Install dependencies\n",
    "pip install ultralytics opencv-python\n",
    "\n",
    "# 3. Run live detection\n",
    "python pi_camera_inference.py\n",
    "```\n",
    "\n",
    "#### **Python Inference (Any Device):**\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your trained model\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "# Run detection\n",
    "results = model('your_image.jpg')\n",
    "results[0].show()  # Display results\n",
    "```\n",
    "\n",
    "### üåü Key Achievements:\n",
    "- ‚ö° **Lightning Fast**: Completed full training pipeline in ~20 minutes\n",
    "- üéØ **Highly Accurate**: State-of-the-art object detection performance  \n",
    "- üì± **Edge Ready**: Optimized for mobile and IoT deployment\n",
    "- üÜì **Cost Effective**: Trained using free Google Colab resources\n",
    "- üîÑ **Reproducible**: Complete pipeline from data to deployment\n",
    "\n",
    "### üìö Learn More:\n",
    "- [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [YOLO Model Deployment Guide](https://docs.ultralytics.com/modes/export/)\n",
    "- [Advanced Training Techniques](https://docs.ultralytics.com/modes/train/)\n",
    "- [DroneBlocks DEXI Platform](https://droneblocks.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Happy detecting with your custom YOLO model! ü§ñ‚ú®**\n",
    "\n",
    "*Built with ‚ù§Ô∏è by DroneBlocks for the DEXI platform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and success message\n",
    "print(\"üéä\" + \"=\"*58 + \"üéä\")\n",
    "print(\"üéâ         YOLO TRAINING COMPLETED SUCCESSFULLY!         üéâ\")\n",
    "print(\"üéä\" + \"=\"*58 + \"üéä\")\n",
    "print()\n",
    "print(\"üèÜ Your custom YOLO model is now trained and ready to deploy!\")\n",
    "print()\n",
    "print(\"üìä Training Summary:\")\n",
    "if 'training_duration' in locals():\n",
    "    print(f\"   ‚è±Ô∏è  Training time: {training_duration/60:.1f} minutes\")\nif 'best_model_path' in locals() and best_model_path:\n",
    "    print(f\"   üíæ Model size: {best_model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "print(f\"   üéØ Classes: bird, dog, cat, motorcycle, car, truck\")\n",
    "print(f\"   üöÄ Device used: {device.upper()}\")\n",
    "print()\n",
    "print(\"üéØ Quick deployment test:\")\n",
    "print(\"```python\")\n",
    "print(\"from ultralytics import YOLO\")\n",
    "print(\"model = YOLO('best.pt')\")\n",
    "print(\"results = model('your_image.jpg')\")\n",
    "print(\"results[0].show()\")\n",
    "print(\"```\")\n",
    "print()\n",
    "print(\"ü•ß Ready for Raspberry Pi drone detection!\")\n",
    "print(\"üåü Built with Google Colab + YOLOv8 + DroneBlocks DEXI\")\n",
    "print()\n",
    "print(\"üéä\" + \"=\"*58 + \"üéä\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygspec_version": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}