{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-header",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DroneBlocks/dexi_yolo_training/blob/main/DEXI_YOLO_Training_Google_Colab.ipynb)\n",
    "\n",
    "# DEXI YOLO Training Tutorial - Google Colab\n",
    "\n",
    "This notebook walks you through the complete process of training a custom YOLO object detection model for drone detection using Google Colab's free GPU. We'll be working with 6 classes: bird, dog, cat, motorcycle, car, and truck.\n",
    "\n",
    "## ğŸš€ Quick Setup:\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ **T4 GPU**\n",
    "2. **Run first cell** â†’ Automatically clones repo and installs everything\n",
    "3. **Run all cells** â†’ Complete YOLO training pipeline in ~20 minutes!\n",
    "\n",
    "## ğŸ“‹ Table of Contents\n",
    "1. [Colab Setup & Repository Clone](#colab-setup)\n",
    "2. [Environment Setup](#environment-setup)\n",
    "3. [Dataset Exploration](#dataset-exploration) \n",
    "4. [Data Augmentation](#data-augmentation)\n",
    "5. [YOLO Training](#yolo-training)\n",
    "6. [Results Analysis](#results-analysis)\n",
    "7. [Model Testing](#model-testing)\n",
    "8. [ONNX Conversion](#onnx-conversion)\n",
    "9. [Download Models](#download-models)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-setup",
   "metadata": {},
   "source": [
    "## 1. Colab Setup & Repository Clone\n",
    "\n",
    "This cell automatically detects if we're running in Google Colab and sets up everything needed:\n",
    "- Clones the DEXI YOLO Training repository\n",
    "- Installs all required dependencies\n",
    "- Pre-downloads the YOLO model\n",
    "\n",
    "**âš ï¸ Important: Enable GPU before running!**\n",
    "- Go to: **Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒŸ GOOGLE COLAB SETUP - Clone Repository & Install Dependencies\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ğŸŒŸ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ğŸ’» Running locally\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository if it doesn't exist\n",
    "    if not Path('dexi_yolo_training').exists():\n",
    "        print(\"ğŸ“¥ Cloning DEXI YOLO Training repository...\")\n",
    "        !git clone https://github.com/DroneBlocks/dexi_yolo_training.git\n",
    "        print(\"âœ… Repository cloned successfully!\")\n",
    "    else:\n",
    "        print(\"ğŸ“ Repository already exists\")\n",
    "    \n",
    "    # Change to repo directory\n",
    "    os.chdir('dexi_yolo_training')\n",
    "    print(f\"ğŸ“‚ Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Install requirements\n",
    "    print(\"ğŸ”§ Installing requirements...\")\n",
    "    !pip install -r requirements.txt -q\n",
    "    \n",
    "    # Pre-download YOLO model to speed up later steps\n",
    "    print(\"ğŸ“¦ Pre-downloading YOLO model...\")\n",
    "    from ultralytics import YOLO\n",
    "    YOLO('yolov8n.pt')  # This downloads and caches the model\n",
    "    \n",
    "    print(\"\\nğŸ‰ Colab setup complete! Ready to train YOLO model.\")\n",
    "    print(\"ğŸ’¡ Make sure GPU is enabled: Runtime â†’ Change runtime type â†’ T4 GPU\")\n",
    "    print(\"ğŸš€ Expected training time with GPU: 15-25 minutes\")\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ’» Local setup detected - skipping Colab-specific steps\")\n",
    "    print(\"ğŸ“ Make sure you have activated your virtual environment\")\n",
    "    print(\"âš¡ Run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-setup",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "Let's check our hardware and import all necessary libraries. This will detect whether we have GPU acceleration available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and check hardware acceleration\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from IPython.display import Image, display\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import glob as globmodule\n",
    "\n",
    "# Set matplotlib style for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"ğŸ”§ System Information:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Enhanced device detection for Colab\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"ğŸš€ GPU: {gpu_name}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Colab-specific GPU info\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(f\"âš¡ Google Colab GPU acceleration enabled!\")\n",
    "        print(f\"   Expected training time: 15-25 minutes\")\n",
    "        print(f\"   Expected augmentation time: 2-3 minutes\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ–¥ï¸  Local CUDA GPU detected\")\n",
    "        \n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps' \n",
    "    print(f\"âš¡ Apple Silicon MPS acceleration!\")\n",
    "    print(f\"   Expected training time: 30-45 minutes\")\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"ğŸ’» Using CPU\")\n",
    "    \n",
    "    # Colab warning if no GPU\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"âš ï¸ WARNING: GPU not enabled in Colab!\")\n",
    "        print(\"ğŸ’¡ Enable GPU: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ T4 GPU\")\n",
    "        print(\"   Then: Runtime â†’ Restart runtime and run all cells again\")\n",
    "    except ImportError:\n",
    "        print(f\"   Expected training time: 2-4 hours\")\n",
    "\n",
    "print(f\"\\nâœ… Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-exploration",
   "metadata": {},
   "source": "## 3. Dataset Exploration\n\nLet's explore our dataset structure and examine the original 6 images that will be used for augmentation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-dataset",
   "metadata": {},
   "outputs": [],
   "source": "# Find and display our original images\noriginal_images_path = Path('train/original_image')\nimage_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n\n# Find all image files\nall_images = []\nfor ext in image_extensions:\n    all_images.extend(original_images_path.glob(ext))\n    all_images.extend(original_images_path.glob(ext.upper()))\n\nprint(f\"ğŸ“¸ Found {len(all_images)} original images in the dataset\")\nprint(f\"ğŸ’¡ These will be augmented to create 900 training images (150 per class)\")\n\n# Display original images\nif all_images:\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    fig.suptitle('Original Images for YOLO Training', fontsize=16, fontweight='bold')\n    \n    for idx, img_path in enumerate(all_images[:6]):\n        if idx >= 6:\n            break\n        \n        row = idx // 3\n        col = idx % 3\n        \n        # Load and display image\n        img = cv2.imread(str(img_path))\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        axes[row, col].imshow(img_rgb)\n        axes[row, col].set_title(f\"{img_path.stem.upper()}\\n{img.shape[1]}x{img.shape[0]}px\", fontweight='bold')\n        axes[row, col].axis('off')\n    \n    # Hide unused subplots\n    for idx in range(len(all_images), 6):\n        row = idx // 3\n        col = idx % 3\n        axes[row, col].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nğŸ“Š Original Images Summary:\")\n    for img_path in all_images[:6]:\n        img = cv2.imread(str(img_path))\n        size_mb = img_path.stat().st_size / (1024*1024)\n        print(f\"  {img_path.name}: {img.shape[1]}x{img.shape[0]} ({size_mb:.1f} MB)\")\n        \nelse:\n    print(\"âŒ No images found in the train/original_image directory\")\n    print(\"ğŸ” Make sure the repository was cloned correctly\")"
  },
  {
   "cell_type": "markdown",
   "id": "data-augmentation",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation\n",
    "\n",
    "Now we'll use our custom augmentation script to create 150 variations of each base image (900 total). This is crucial for training a robust YOLO model as it helps the model generalize better to different conditions.\n",
    "\n",
    "**â±ï¸ Expected time: 2-3 minutes with GPU, 5-7 minutes with CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation-overview",
   "metadata": {},
   "outputs": [],
   "source": "# Let's examine our augmentation script first\nprint(\"ğŸ”§ Data Augmentation Pipeline Overview:\")\nprint(\"\")\nprint(\"Our augmentation script applies the following transformations:\")\nprint(\"â€¢ ğŸ”„ Rotation: 0-360 degrees (random)\")\nprint(\"â€¢ ğŸ“ Scaling: 0.25x to 1.3x (random)\")\nprint(\"â€¢ â˜€ï¸ Brightness: -30 to +30 (random)\")\nprint(\"â€¢ ğŸŒˆ Contrast: 0.7x to 1.3x (random)\")\nprint(\"â€¢ ğŸ“» Noise: Added 20% of the time\")\nprint(\"â€¢ ğŸŒ«ï¸ Blur: Applied 15% of the time\")\nprint(\"\")\nprint(\"Each transformation creates realistic variations that help the model\")\nprint(\"learn to detect objects under different lighting, weather, and camera conditions.\")\nprint(\"\")\nprint(\"ğŸ’¡ This technique dramatically improves model robustness with minimal data!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation-config",
   "metadata": {},
   "outputs": [],
   "source": "# Set augmentation parameters\nAUGMENTATIONS_PER_IMAGE = 150  # Creates 900 total images (6 classes Ã— 150)\nINPUT_DIR = \"train/original_image\"     # Directory with original images\nOUTPUT_DIR = \"train\"          # Output directory for augmented dataset\n\nprint(f\"âš™ï¸ Augmentation Configuration:\")\nprint(f\"   Input directory: {INPUT_DIR}\")\nprint(f\"   Output directory: {OUTPUT_DIR}\")\nprint(f\"   Augmentations per image: {AUGMENTATIONS_PER_IMAGE}\")\nprint(f\"   Expected total images: {len(all_images) * AUGMENTATIONS_PER_IMAGE}\")\nprint(f\"\")\nprint(f\"ğŸ“Š Training Dataset Breakdown:\")\nfor img_path in all_images[:6]:\n    class_name = img_path.stem\n    print(f\"   {class_name}: {AUGMENTATIONS_PER_IMAGE} augmented images\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the augmentation process\n",
    "print(\"ğŸš€ Starting data augmentation...\")\n",
    "\n",
    "# Time estimation based on environment\n",
    "if device == 'cuda':\n",
    "    print(\"âš¡ GPU acceleration detected - estimated time: 2-3 minutes\")\n",
    "elif device == 'mps':\n",
    "    print(\"ğŸ Apple Silicon detected - estimated time: 3-4 minutes\")\n",
    "else:\n",
    "    print(\"ğŸ’» CPU processing - estimated time: 5-7 minutes\")\n",
    "\n",
    "print(\"ğŸ“ˆ Progress will be shown below...\")\n",
    "print(\"\")\n",
    "\n",
    "# Import and use our augmentation class\n",
    "from augment_dataset import YOLODatasetAugmenter\n",
    "\n",
    "# Create augmenter instance\n",
    "augmenter = YOLODatasetAugmenter(INPUT_DIR, OUTPUT_DIR)\n",
    "\n",
    "# Run augmentation with progress tracking\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "augmenter.augment_all_images(AUGMENTATIONS_PER_IMAGE)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\nâœ… Data augmentation completed in {duration/60:.1f} minutes!\")\n",
    "print(f\"ğŸ¯ Ready for YOLO training with {len(all_images) * AUGMENTATIONS_PER_IMAGE} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify augmentation results\n",
    "train_images_dir = Path('train/images')\n",
    "train_labels_dir = Path('train/labels')\n",
    "\n",
    "# Count generated files\n",
    "augmented_images = list(train_images_dir.glob('*_[0-9][0-9][0-9].jpg'))\n",
    "augmented_labels = list(train_labels_dir.glob('*_[0-9][0-9][0-9].txt'))\n",
    "original_images = [f for f in train_images_dir.glob('*.jpg') if not f.name.endswith(('_001.jpg', '_002.jpg', '_003.jpg'))]\n",
    "\n",
    "print(f\"ğŸ“Š Augmentation Results Summary:\")\n",
    "print(f\"   Original base images: {len(original_images)}\")\n",
    "print(f\"   Generated images: {len(augmented_images)}\")\n",
    "print(f\"   Total training images: {len(list(train_images_dir.glob('*.jpg')))}\")\n",
    "print(f\"   Label files created: {len(augmented_labels)}\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ¯ Images per class (including originals):\")\n",
    "for class_name in ['bird', 'dog', 'cat', 'motorcycle', 'car', 'truck']:\n",
    "    class_images = len(list(train_images_dir.glob(f'{class_name}*.jpg')))\n",
    "    print(f\"   {class_name.title()}: {class_images} images\")\n",
    "\n",
    "# Verify dataset balance\n",
    "total_images = len(list(train_images_dir.glob('*.jpg')))\n",
    "if total_images == 906:  # 6 original + 900 augmented\n",
    "    print(f\"\\nâœ… Dataset verification passed! Ready for training.\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Expected 906 images, found {total_images}. Check augmentation process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-augmented-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples of augmented images\n",
    "print(\"ğŸ–¼ï¸ Sample Augmented Images (Showing Transformations):\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Augmented Training Images - Variety of Transformations', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show examples from each class\n",
    "classes = ['bird', 'dog', 'cat', 'motorcycle', 'car', 'truck']\n",
    "sample_count = 0\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    # Get first 2 augmented images of each class\n",
    "    class_images = list(train_images_dir.glob(f'{class_name}_*.jpg'))[:2]\n",
    "    \n",
    "    for img_idx, img_path in enumerate(class_images):\n",
    "        if sample_count >= 12:  # 3x4 grid\n",
    "            break\n",
    "            \n",
    "        row = sample_count // 4\n",
    "        col = sample_count % 4\n",
    "        \n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[row, col].imshow(img_rgb)\n",
    "        axes[row, col].set_title(f\"{class_name.title()}\\n{img_path.name}\", fontweight='bold', fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        sample_count += 1\n",
    "    \n",
    "    if sample_count >= 12:\n",
    "        break\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(sample_count, 12):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¨ Notice the variety in rotations, brightness, and transformations!\")\n",
    "print(\"ğŸ§  This diversity helps the model generalize to real-world conditions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yolo-training",
   "metadata": {},
   "source": [
    "## 5. YOLO Training\n",
    "\n",
    "Now that we have our augmented dataset ready, let's train our YOLO model! We'll use the YOLOv8 nano architecture, which is perfect for edge deployment while still achieving excellent accuracy.\n",
    "\n",
    "**â±ï¸ Expected time: 15-25 minutes with T4 GPU, 30-45 minutes with local GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration optimized for Colab\n",
    "TRAINING_CONFIG = {\n",
    "    'model_size': 'n',        # Nano: Fast, efficient, perfect for Pi deployment\n",
    "    'epochs': 100,            # Number of training epochs\n",
    "    'imgsz': 640,             # Image size for training\n",
    "    'batch_size': 16 if device == 'cuda' else 8,  # Adjust for GPU memory\n",
    "    'device': device,         # Device determined earlier\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(\"\\nğŸ’¡ YOLOv8 Model Size Guide:\")\n",
    "print(\"   â€¢ 'n' (nano): âš¡ Fastest, smallest, perfect for Pi/mobile deployment\")\n",
    "print(\"   â€¢ 's' (small): ğŸ”„ Good balance of speed and accuracy\")\n",
    "print(\"   â€¢ 'm' (medium): ğŸ¯ Better accuracy, moderate speed\")\n",
    "print(\"   â€¢ 'l' (large): ğŸ† High accuracy, slower inference\")\n",
    "print(\"   â€¢ 'x' (extra-large): ğŸ¥‡ Highest accuracy, slowest inference\")\n",
    "\n",
    "print(f\"\\nâš¡ Performance Expectations ({device.upper()}):\")\n",
    "if device == 'cuda':\n",
    "    print(\"   Training time: ~15-25 minutes\")\n",
    "    print(\"   Expected mAP@0.5: 99%+ (excellent!)\")\n",
    "elif device == 'mps':\n",
    "    print(\"   Training time: ~30-45 minutes\") \n",
    "    print(\"   Expected mAP@0.5: 99%+ (excellent!)\")\n",
    "else:\n",
    "    print(\"   Training time: 2-4 hours\")\n",
    "    print(\"   Consider enabling GPU for much faster training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the YOLO model\n",
    "model_name = f\"yolov8{TRAINING_CONFIG['model_size']}.pt\"\n",
    "print(f\"ğŸ¤– Loading {model_name} model...\")\n",
    "\n",
    "# Load pre-trained YOLO model\n",
    "model = YOLO(model_name)\n",
    "\n",
    "print(f\"\\nâœ… Model loaded successfully!\")\n",
    "print(f\"   Model: YOLOv8{TRAINING_CONFIG['model_size']}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.model.parameters()):,}\")\n",
    "print(f\"   Model size: {os.path.getsize(model_name) / (1024*1024):.1f} MB\")\n",
    "print(f\"   Perfect for Raspberry Pi deployment! ğŸ¥§\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": "# Start training with progress monitoring\nprint(\"ğŸš€ Starting YOLO training...\")\nprint(\"ğŸ“Š You'll see training progress, loss curves, and validation metrics below.\")\nprint(\"ğŸ’¾ Training logs and checkpoints will be saved in 'runs/detect/drone_detection/'\")\nprint(\"ğŸ’¡ YOLO will automatically split training data for validation (80% train, 20% val)\")\nprint(\"\")\n\n# Time estimation\nif device == 'cuda':\n    print(\"â±ï¸  Estimated completion: 15-25 minutes\")\n    print(\"â˜• Perfect time for a coffee break!\")\nelif device == 'mps':\n    print(\"â±ï¸  Estimated completion: 30-45 minutes\")\n    print(\"ğŸµ Great time for tea and checking emails!\")\nelse:\n    print(\"â±ï¸  Estimated completion: 2-4 hours\")\n    print(\"ğŸ’¡ Consider enabling GPU: Runtime â†’ Change runtime type â†’ T4 GPU\")\n\nprint(\"\\nğŸ¯ Training starting...\")\nprint(\"=\" * 60)\n\nimport time\ntraining_start_time = time.time()\n\n# Train the model with optimized parameters\nresults = model.train(\n    data='dataset.yaml',\n    epochs=TRAINING_CONFIG['epochs'],\n    imgsz=TRAINING_CONFIG['imgsz'],\n    batch=TRAINING_CONFIG['batch_size'],\n    device=TRAINING_CONFIG['device'],\n    project='runs/detect',\n    name='drone_detection',\n    save_period=20,      # Save checkpoint every 20 epochs (less frequent for Colab)\n    patience=25,         # Early stopping patience\n    \n    # Automatic train/validation split (no separate val folder needed)\n    fraction=0.8,        # 80% for training, 20% for validation\n    \n    # Augmentation settings (additional to our pre-generated augmentations)\n    hsv_h=0.015,         # Hue augmentation\n    hsv_s=0.7,           # Saturation augmentation  \n    hsv_v=0.4,           # Value augmentation\n    degrees=0,           # No additional rotation (we already did this)\n    translate=0.1,       # Translation augmentation\n    scale=0.1,           # Additional scale augmentation\n    shear=0.1,           # Shear augmentation\n    perspective=0.0,     # Perspective augmentation\n    flipud=0.0,          # No vertical flip (objects have orientation)\n    fliplr=0.0,          # No horizontal flip (for consistency)\n    mosaic=0.8,          # Mosaic augmentation probability\n    mixup=0.1,           # Mixup augmentation probability\n    \n    # Optimization settings\n    optimizer='AdamW',   # Adam with weight decay\n    lr0=0.01,            # Initial learning rate\n    lrf=0.1,             # Final learning rate (lr0 * lrf)\n    momentum=0.937,      # SGD momentum\n    weight_decay=0.0005, # Weight decay for regularization\n    warmup_epochs=3,     # Warm-up epochs\n    warmup_momentum=0.8, # Warm-up momentum\n    warmup_bias_lr=0.1,  # Warm-up bias learning rate\n    \n    # Loss function weights\n    box=7.5,             # Box loss gain\n    cls=0.5,             # Class loss gain  \n    dfl=1.5,             # DFL loss gain\n    verbose=True,        # Verbose output\n)\n\ntraining_end_time = time.time()\ntraining_duration = training_end_time - training_start_time\n\nprint(\"=\" * 60)\nprint(f\"ğŸ‰ Training completed in {training_duration/60:.1f} minutes!\")\nprint(f\"ğŸ† Model ready for deployment!\")"
  },
  {
   "cell_type": "markdown",
   "id": "results-analysis",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "Let's analyze the training results and visualize the model's performance with charts and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest training results directory\n",
    "results_dirs = globmodule.glob('runs/detect/drone_detection*')\n",
    "if results_dirs:\n",
    "    results_dir = Path(max(results_dirs, key=os.path.getmtime))\n",
    "    print(f\"ğŸ“‚ Latest training results: {results_dir}\")\n",
    "    print(f\"ğŸ“ Training artifacts:\")\n",
    "    for item in sorted(results_dir.iterdir()):\n",
    "        if item.is_file():\n",
    "            print(f\"   ğŸ“„ {item.name}\")\n",
    "        else:\n",
    "            print(f\"   ğŸ“ {item.name}/\")\n",
    "else:\n",
    "    print(\"âŒ Training results not found. Make sure training completed successfully.\")\n",
    "    results_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-training-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training curves\n",
    "if results_dir and results_dir.exists():\n",
    "    results_image_path = results_dir / 'results.png'\n",
    "    if results_image_path.exists():\n",
    "        print(\"ğŸ“ˆ Training Results & Performance Curves:\")\n",
    "        print(\"These show how the model learned over time - loss should decrease, mAP should increase!\")\n",
    "        display(Image(str(results_image_path)))\n",
    "        \n",
    "        print(\"\\nğŸ“Š How to read these charts:\")\n",
    "        print(\"â€¢ ğŸ“‰ Loss curves (lower is better): Shows training progress\")\n",
    "        print(\"â€¢ ğŸ“ˆ mAP curves (higher is better): Shows detection accuracy\")\n",
    "        print(\"â€¢ ğŸ¯ Precision/Recall: Shows detection quality\")\n",
    "        print(\"â€¢ âš¡ Look for: Decreasing loss, increasing mAP > 0.9\")\n",
    "    else:\n",
    "        print(\"ğŸ“ˆ Training curves not found. They should be available after training completes.\")\n",
    "else:\n",
    "    print(\"âŒ No results directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "if results_dir and results_dir.exists():\n",
    "    confusion_matrix_path = results_dir / 'confusion_matrix.png'\n",
    "    if confusion_matrix_path.exists():\n",
    "        print(\"ğŸ¯ Confusion Matrix - Model Classification Accuracy:\")\n",
    "        print(\"This shows how well the model distinguishes between different classes.\")\n",
    "        print(\"Perfect diagonal = 100% accuracy for each class!\")\n",
    "        display(Image(str(confusion_matrix_path)))\n",
    "        \n",
    "        print(\"\\nğŸ“Š How to read the confusion matrix:\")\n",
    "        print(\"â€¢ Diagonal (top-left to bottom-right): Correct predictions\")\n",
    "        print(\"â€¢ Off-diagonal: Misclassifications between classes\")\n",
    "        print(\"â€¢ Darker blue = higher values = better performance\")\n",
    "        print(\"â€¢ Goal: Dark diagonal, light off-diagonal areas\")\n",
    "    else:\n",
    "        print(\"ğŸ¯ Confusion matrix not found.\")\n",
    "else:\n",
    "    print(\"âŒ No results directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display validation batch results\n",
    "if results_dir and results_dir.exists():\n",
    "    val_batch_path = results_dir / 'val_batch0_labels.jpg'\n",
    "    val_pred_path = results_dir / 'val_batch0_pred.jpg'\n",
    "    \n",
    "    if val_batch_path.exists():\n",
    "        print(\"ğŸ” Validation Batch - Ground Truth Labels:\")\n",
    "        print(\"These are the 'correct answers' the model should learn.\")\n",
    "        display(Image(str(val_batch_path)))\n",
    "    \n",
    "    if val_pred_path.exists():\n",
    "        print(\"\\nğŸ¤– Validation Batch - Model Predictions:\")\n",
    "        print(\"These are what the model actually predicted. Compare with ground truth above!\")\n",
    "        print(\"Good predictions = boxes in same locations with correct labels and high confidence.\")\n",
    "        display(Image(str(val_pred_path)))\n",
    "        \n",
    "        if val_batch_path.exists():\n",
    "            print(\"\\nğŸ’¡ Comparison Tips:\")\n",
    "            print(\"â€¢ âœ… Bounding boxes should align well between ground truth and predictions\")\n",
    "            print(\"â€¢ âœ… Class labels should match (bird, car, dog, etc.)\")\n",
    "            print(\"â€¢ âœ… Confidence scores should be high (>0.5, ideally >0.8)\")\n",
    "            print(\"â€¢ ğŸ¯ Perfect alignment = excellent model performance!\")\n",
    "\n",
    "    if not val_batch_path.exists() and not val_pred_path.exists():\n",
    "        print(\"ğŸ” Validation images not found.\")\n",
    "else:\n",
    "    print(\"âŒ No results directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and run final validation\n",
    "if results_dir and results_dir.exists():\n",
    "    best_model_path = results_dir / 'weights' / 'best.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"ğŸ† Loading best trained model: {best_model_path}\")\n",
    "        trained_model = YOLO(str(best_model_path))\n",
    "        \n",
    "        print(f\"ğŸ“Š Model file size: {best_model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"ğŸ¥§ Perfect size for Raspberry Pi deployment!\")\n",
    "        \n",
    "        # Run final validation\n",
    "        print(\"\\nğŸ”¬ Running final validation...\")\n",
    "        val_results = trained_model.val()\n",
    "        \n",
    "        # Print key metrics with explanations\n",
    "        print(\"\\nğŸ¯ Final Model Performance Metrics:\")\n",
    "        print(\"=\" * 50)\n",
    "        if hasattr(val_results, 'box'):\n",
    "            metrics = val_results.box\n",
    "            map50 = metrics.map50\n",
    "            map_5095 = metrics.map\n",
    "            precision = metrics.mp\n",
    "            recall = metrics.mr\n",
    "            \n",
    "            print(f\"ğŸ“ˆ mAP@0.5:      {map50:.3f} ({'ğŸ¥‡ Excellent!' if map50 > 0.95 else 'ğŸ¥ˆ Very Good!' if map50 > 0.90 else 'ğŸ¥‰ Good!' if map50 > 0.80 else 'âš ï¸ Needs improvement'})\")\n",
    "            print(f\"ğŸ“ˆ mAP@0.5:0.95: {map_5095:.3f} ({'ğŸ¥‡ Excellent!' if map_5095 > 0.90 else 'ğŸ¥ˆ Very Good!' if map_5095 > 0.80 else 'ğŸ¥‰ Good!' if map_5095 > 0.70 else 'âš ï¸ Needs improvement'})\")\n",
    "            print(f\"ğŸ¯ Precision:    {precision:.3f} (How accurate are detections?)\")\n",
    "            print(f\"ğŸ” Recall:       {recall:.3f} (How many objects were found?)\")\n",
    "            \n",
    "            print(\"\\nğŸ’¡ Metric Explanations:\")\n",
    "            print(\"â€¢ mAP@0.5: Average precision at 50% overlap threshold\")\n",
    "            print(\"â€¢ mAP@0.5:0.95: Average precision across multiple thresholds (more strict)\")\n",
    "            print(\"â€¢ Precision: Of all detections, how many were correct?\")\n",
    "            print(\"â€¢ Recall: Of all ground truth objects, how many were detected?\")\n",
    "            \n",
    "            # Performance assessment\n",
    "            if map50 > 0.95 and map_5095 > 0.90:\n",
    "                print(\"\\nğŸ‰ OUTSTANDING PERFORMANCE! This model is ready for production deployment.\")\n",
    "            elif map50 > 0.90 and map_5095 > 0.80:\n",
    "                print(\"\\nâœ… EXCELLENT PERFORMANCE! This model will work very well in real-world scenarios.\")\n",
    "            elif map50 > 0.80 and map_5095 > 0.70:\n",
    "                print(\"\\nğŸ‘ GOOD PERFORMANCE! This model should work well for most use cases.\")\n",
    "            else:\n",
    "                print(\"\\nâš ï¸ Model performance could be improved. Consider more training data or different parameters.\")\n",
    "                \n",
    "        else:\n",
    "            print(\"âŒ Could not extract validation metrics.\")\n",
    "    else:\n",
    "        print(\"âŒ Best model not found. Training may not have completed successfully.\")\n",
    "        trained_model = None\n",
    "        best_model_path = None\n",
    "else:\n",
    "    print(\"âŒ No results directory found.\")\n",
    "    trained_model = None\n",
    "    best_model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-testing",
   "metadata": {},
   "source": [
    "## 7. Model Testing\n",
    "\n",
    "Let's test our trained model on some sample images to see how well it performs in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on sample training images\n",
    "if 'trained_model' in locals() and trained_model is not None:\n",
    "    print(\"ğŸ§ª Testing the trained model on sample images...\")\n",
    "    print(\"This shows how well the model detects and classifies objects!\")\n",
    "    \n",
    "    # Get some test images (first augmented image of each class)\n",
    "    train_images_dir = Path('train/images')\n",
    "    test_images = list(train_images_dir.glob('*_001.jpg'))[:6]\n",
    "    \n",
    "    if test_images:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('ğŸ” Model Predictions on Test Images', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for idx, img_path in enumerate(test_images[:6]):\n",
    "            row = idx // 3\n",
    "            col = idx % 3\n",
    "            \n",
    "            # Run inference\n",
    "            results = trained_model(str(img_path), verbose=False, conf=0.5)\n",
    "            \n",
    "            # Get the annotated image with predictions\n",
    "            annotated_img = results[0].plot()\n",
    "            annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[row, col].imshow(annotated_img_rgb)\n",
    "            axes[row, col].set_title(f\"Test: {img_path.stem}\", fontweight='bold')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nğŸ¯ What to look for in the predictions:\")\n",
    "        print(\"â€¢ âœ… Bounding boxes around objects (colored rectangles)\")\n",
    "        print(\"â€¢ ğŸ·ï¸  Class labels (bird, car, dog, cat, motorcycle, truck)\")\n",
    "        print(\"â€¢ ğŸ“Š Confidence scores (higher = more confident, >0.5 is good)\")\n",
    "        print(\"â€¢ ğŸ¨ Different colors for different classes\")\n",
    "        print(\"\\nğŸ’¡ High confidence + correct labels = excellent model performance!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No test images found.\")\n",
    "else:\n",
    "    print(\"âŒ Trained model not available. Please complete the training step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-test-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test model on any image (for future use)\n",
    "def test_on_custom_image(image_path, confidence_threshold=0.5):\n",
    "    \"\"\"Test the trained model on a custom image\"\"\"\n",
    "    if 'trained_model' not in locals() or trained_model is None:\n",
    "        print(\"âŒ Trained model not available. Please complete the training step first.\")\n",
    "        return\n",
    "    \n",
    "    if not Path(image_path).exists():\n",
    "        print(f\"âŒ Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ” Testing model on: {image_path}\")\n",
    "    \n",
    "    # Run inference\n",
    "    results = trained_model(image_path, conf=confidence_threshold, verbose=False)\n",
    "    \n",
    "    # Display results\n",
    "    annotated_img = results[0].plot()\n",
    "    annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(annotated_img_rgb)\n",
    "    plt.title(f'ğŸ¯ Detection Results - {Path(image_path).name}', fontweight='bold', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load dataset config to get class names\n",
    "    with open('dataset.yaml', 'r') as f:\n",
    "        dataset_config = yaml.safe_load(f)\n",
    "    \n",
    "    # Print detection details\n",
    "    if len(results[0].boxes) > 0:\n",
    "        print(\"\\nğŸ¯ Detections found:\")\n",
    "        for i, box in enumerate(results[0].boxes):\n",
    "            class_id = int(box.cls[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            class_name = dataset_config['names'][class_id]\n",
    "            print(f\"   {i+1}. {class_name.upper()} (confidence: {confidence:.3f})\")\n",
    "    else:\n",
    "        print(\"\\nâŒ No objects detected above confidence threshold\")\n",
    "        print(f\"ğŸ’¡ Try lowering confidence_threshold (current: {confidence_threshold})\")\n",
    "\n",
    "print(\"ğŸ“ Custom testing function created!\")\n",
    "print(\"ğŸ’¡ Usage example:\")\n",
    "print(\"   test_on_custom_image('path/to/your/image.jpg', confidence_threshold=0.3)\")\n",
    "print(\"   (Upload your own images to test the model!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "onnx-conversion",
   "metadata": {},
   "source": [
    "## 8. ONNX Conversion for Deployment\n",
    "\n",
    "Let's convert our trained PyTorch model to ONNX format for efficient deployment on devices like Raspberry Pi. ONNX models are faster and more compatible across different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onnx-conversion-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to ONNX format optimized for Pi deployment\n",
    "if 'best_model_path' in locals() and best_model_path and best_model_path.exists():\n",
    "    print(\"ğŸš€ Converting trained PyTorch model to ONNX format...\")\n",
    "    print(\"ğŸ¥§ Optimizing for Raspberry Pi deployment!\")\n",
    "    \n",
    "    try:\n",
    "        # Use built-in YOLO export function (simpler and more reliable than custom script)\n",
    "        print(\"\\nâš™ï¸ Export Configuration:\")\n",
    "        print(\"   Format: ONNX\")\n",
    "        print(\"   Image size: 320x320 (optimized for Pi camera)\")\n",
    "        print(\"   Precision: FP32 (better Pi compatibility)\")\n",
    "        print(\"   Simplification: Enabled (faster inference)\")\n",
    "        \n",
    "        # Export the model\n",
    "        onnx_path = trained_model.export(\n",
    "            format='onnx',\n",
    "            imgsz=320,        # Smaller size for Pi camera (320x240)\n",
    "            half=False,       # Use FP32 for better Pi compatibility\n",
    "            simplify=True,    # Simplify the ONNX graph for better performance\n",
    "            opset=11,         # ONNX opset version (compatible with most systems)\n",
    "        )\n",
    "        \n",
    "        if onnx_path and Path(onnx_path).exists():\n",
    "            print(f\"\\nâœ… ONNX conversion successful!\")\n",
    "            print(f\"ğŸ“ ONNX model saved: {onnx_path}\")\n",
    "            \n",
    "            # Show file sizes\n",
    "            pytorch_size = best_model_path.stat().st_size / (1024*1024)\n",
    "            onnx_size = Path(onnx_path).stat().st_size / (1024*1024)\n",
    "            print(f\"ğŸ“Š File size comparison:\")\n",
    "            print(f\"   PyTorch (.pt): {pytorch_size:.1f} MB\")\n",
    "            print(f\"   ONNX (.onnx):  {onnx_size:.1f} MB\")\n",
    "            \n",
    "            # Test the ONNX model\n",
    "            try:\n",
    "                import onnxruntime as ort\n",
    "                session = ort.InferenceSession(onnx_path)\n",
    "                inputs = session.get_inputs()\n",
    "                outputs = session.get_outputs()\n",
    "                \n",
    "                print(f\"\\nğŸ” ONNX Model Verification:\")\n",
    "                print(f\"   âœ… Model loads successfully\")\n",
    "                print(f\"   ğŸ“ Input shape: {inputs[0].shape}\")\n",
    "                print(f\"   ğŸ“Š Output shape: {outputs[0].shape}\")\n",
    "                print(f\"   ğŸ–¥ï¸  Available providers: {ort.get_available_providers()}\")\n",
    "                \n",
    "                print(f\"\\nğŸ‰ Model ready for Raspberry Pi deployment!\")\n",
    "                \n",
    "            except ImportError:\n",
    "                print(f\"\\nâš ï¸  ONNX Runtime not available for verification\")\n",
    "                print(f\"   Model conversion completed, but couldn't verify\")\n",
    "                print(f\"   Install with: pip install onnxruntime\")\n",
    "                \n",
    "        else:\n",
    "            print(\"âŒ ONNX conversion failed - file not created\")\n",
    "            onnx_path = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ONNX conversion failed: {e}\")\n",
    "        print(f\"ğŸ’¡ The PyTorch model still works perfectly for most deployments\")\n",
    "        onnx_path = None\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No trained model found. Please complete training first.\")\n",
    "    onnx_path = None\n",
    "\n",
    "# Deployment instructions\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ¥§ RASPBERRY PI DEPLOYMENT INSTRUCTIONS\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"\"\"ğŸ“‹ Quick setup steps:\n",
    "\n",
    "1. ğŸ“¥ Download your model files (next cell)\n",
    "2. ğŸ“¤ Copy to your Raspberry Pi:\n",
    "   scp best.pt pi@your-pi-ip:~/\n",
    "   \n",
    "3. ğŸ”§ Install dependencies on Pi:\n",
    "   pip install ultralytics opencv-python\n",
    "   \n",
    "4. ğŸ Python inference code:\n",
    "   ```python\n",
    "   from ultralytics import YOLO\n",
    "   model = YOLO('best.pt')\n",
    "   results = model('image.jpg')\n",
    "   results[0].show()\n",
    "   ```\n",
    "\n",
    "5. ğŸ“¹ For live camera detection:\n",
    "   Use the pi_camera_inference.py script included in the repo!\n",
    "   \n",
    "ğŸ¯ Classes: bird, dog, cat, motorcycle, car, truck\n",
    "âš¡ Expected Pi performance: 2-5 FPS (great for real-time detection!)\n",
    "\"\"\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-models",
   "metadata": {},
   "source": [
    "## 9. Download Your Trained Models\n",
    "\n",
    "ğŸ‰ **Congratulations!** Your YOLO model is trained and ready for deployment. Let's download the model files so you can use them on your Raspberry Pi or other devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-models-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained models (Colab only)\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"ğŸ“¥ Preparing model downloads for local use...\")\n",
    "    print(\"ğŸ¯ These files are ready for Raspberry Pi deployment!\")\n",
    "    \n",
    "    download_count = 0\n",
    "    \n",
    "    # Download the best PyTorch model\n",
    "    if 'best_model_path' in locals() and best_model_path and best_model_path.exists():\n",
    "        print(f\"\\nğŸ“¦ Downloading: {best_model_path.name}\")\n",
    "        print(f\"   Size: {best_model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"   Use: Primary model for inference\")\n",
    "        files.download(str(best_model_path))\n",
    "        download_count += 1\n",
    "        \n",
    "        # Also download the last model (in case best model has issues)\n",
    "        last_model_path = best_model_path.parent / 'last.pt'\n",
    "        if last_model_path.exists():\n",
    "            print(f\"\\nğŸ“¦ Downloading: {last_model_path.name} (backup)\")\n",
    "            print(f\"   Size: {last_model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "            print(f\"   Use: Alternative if best.pt has issues\")\n",
    "            files.download(str(last_model_path))\n",
    "            download_count += 1\n",
    "    \n",
    "    # Download ONNX model if it exists\n",
    "    if 'onnx_path' in locals() and onnx_path and Path(onnx_path).exists():\n",
    "        print(f\"\\nğŸ“¦ Downloading: {Path(onnx_path).name}\")\n",
    "        print(f\"   Size: {Path(onnx_path).stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"   Use: Optimized for faster inference\")\n",
    "        files.download(str(onnx_path))\n",
    "        download_count += 1\n",
    "    \n",
    "    # Download dataset config\n",
    "    dataset_config_path = Path('dataset.yaml')\n",
    "    if dataset_config_path.exists():\n",
    "        print(f\"\\nğŸ“¦ Downloading: {dataset_config_path.name}\")\n",
    "        print(f\"   Use: Contains class names and dataset info\")\n",
    "        files.download(str(dataset_config_path))\n",
    "        download_count += 1\n",
    "    \n",
    "    # Download Pi inference script\n",
    "    pi_script_path = Path('pi_camera_inference.py')\n",
    "    if pi_script_path.exists():\n",
    "        print(f\"\\nğŸ“¦ Downloading: {pi_script_path.name}\")\n",
    "        print(f\"   Use: Ready-to-run Pi camera detection script\")\n",
    "        files.download(str(pi_script_path))\n",
    "        download_count += 1\n",
    "    \n",
    "    if download_count > 0:\n",
    "        print(f\"\\nâœ… Downloaded {download_count} files to your browser's Downloads folder!\")\n",
    "        print(f\"ğŸš€ Your YOLO model is ready for deployment!\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Quick Start on Raspberry Pi:\")\n",
    "        print(f\"1. Copy best.pt to your Pi\")\n",
    "        print(f\"2. Install: pip install ultralytics opencv-python\")\n",
    "        print(f\"3. Run: python pi_camera_inference.py\")\n",
    "        print(f\"4. Point camera at objects and watch the magic! ğŸª„\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No model files found to download.\")\n",
    "        print(\"ğŸ” Make sure training completed successfully.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Local environment detected - no downloads needed\")\n",
    "    print(\"ğŸ—‚ï¸  Your trained models are saved in:\")\n",
    "    if 'results_dir' in locals() and results_dir:\n",
    "        print(f\"   ğŸ“ {results_dir}/weights/\")\n",
    "        print(f\"   ğŸ“„ best.pt (main model)\")\n",
    "        print(f\"   ğŸ“„ last.pt (backup model)\")\n",
    "        if 'onnx_path' in locals() and onnx_path:\n",
    "            print(f\"   ğŸ“„ {Path(onnx_path).name} (ONNX version)\")\n",
    "    else:\n",
    "        print(\"   ğŸ” Check runs/detect/drone_detection*/weights/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations! Mission Accomplished!\n",
    "\n",
    "You've successfully completed the **DEXI YOLO Training Pipeline** in Google Colab! ğŸš€\n",
    "\n",
    "### âœ… What You've Accomplished:\n",
    "\n",
    "1. **ğŸŒŸ Environment Setup**: Automatically cloned repo and configured Colab with GPU acceleration\n",
    "2. **ğŸ“Š Dataset Exploration**: Examined your 6 base images for training\n",
    "3. **ğŸ”„ Data Augmentation**: Generated 900 diverse training images from just 6 originals\n",
    "4. **ğŸ¤– YOLO Training**: Trained a custom YOLOv8 nano model with excellent accuracy\n",
    "5. **ğŸ“ˆ Results Analysis**: Visualized training curves, confusion matrix, and validation results\n",
    "6. **ğŸ§ª Model Testing**: Verified predictions on test images\n",
    "7. **âš¡ ONNX Conversion**: Optimized model for fast Raspberry Pi deployment\n",
    "8. **ğŸ“¥ Model Download**: Downloaded all files needed for deployment\n",
    "\n",
    "### ğŸ† Your Model Performance:\n",
    "- **ğŸ¯ Detection Classes**: Bird, Dog, Cat, Motorcycle, Car, Truck\n",
    "- **ğŸ“Š Expected Accuracy**: 99%+ mAP@0.5 (exceptional!)\n",
    "- **âš¡ Model Size**: ~6MB (perfect for edge devices)\n",
    "- **ğŸ¥§ Pi Performance**: 2-5 FPS real-time detection\n",
    "\n",
    "### ğŸš€ Next Steps - Deploy Your Model:\n",
    "\n",
    "#### **Raspberry Pi Deployment:**\n",
    "```bash\n",
    "# 1. Copy your model to Pi\n",
    "scp best.pt pi@your-pi-ip:~/\n",
    "\n",
    "# 2. Install dependencies\n",
    "pip install ultralytics opencv-python\n",
    "\n",
    "# 3. Run live detection\n",
    "python pi_camera_inference.py\n",
    "```\n",
    "\n",
    "#### **Python Inference (Any Device):**\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your trained model\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "# Run detection\n",
    "results = model('your_image.jpg')\n",
    "results[0].show()  # Display results\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Key Achievements:\n",
    "- âš¡ **Lightning Fast**: Completed full training pipeline in ~20 minutes\n",
    "- ğŸ¯ **Highly Accurate**: State-of-the-art object detection performance  \n",
    "- ğŸ“± **Edge Ready**: Optimized for mobile and IoT deployment\n",
    "- ğŸ†“ **Cost Effective**: Trained using free Google Colab resources\n",
    "- ğŸ”„ **Reproducible**: Complete pipeline from data to deployment\n",
    "\n",
    "### ğŸ“š Learn More:\n",
    "- [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [YOLO Model Deployment Guide](https://docs.ultralytics.com/modes/export/)\n",
    "- [Advanced Training Techniques](https://docs.ultralytics.com/modes/train/)\n",
    "- [DroneBlocks DEXI Platform](https://droneblocks.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ Happy detecting with your custom YOLO model! ğŸ¤–âœ¨**\n",
    "\n",
    "*Built with â¤ï¸ by DroneBlocks for the DEXI platform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and success message\n",
    "print(\"ğŸŠ\" + \"=\"*58 + \"ğŸŠ\")\n",
    "print(\"ğŸ‰         YOLO TRAINING COMPLETED SUCCESSFULLY!         ğŸ‰\")\n",
    "print(\"ğŸŠ\" + \"=\"*58 + \"ğŸŠ\")\n",
    "print()\n",
    "print(\"ğŸ† Your custom YOLO model is now trained and ready to deploy!\")\n",
    "print()\n",
    "print(\"ğŸ“Š Training Summary:\")\n",
    "if 'training_duration' in locals():\n",
    "    print(f\"   â±ï¸  Training time: {training_duration/60:.1f} minutes\")\nif 'best_model_path' in locals() and best_model_path:\n",
    "    print(f\"   ğŸ’¾ Model size: {best_model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "print(f\"   ğŸ¯ Classes: bird, dog, cat, motorcycle, car, truck\")\n",
    "print(f\"   ğŸš€ Device used: {device.upper()}\")\n",
    "print()\n",
    "print(\"ğŸ¯ Quick deployment test:\")\n",
    "print(\"```python\")\n",
    "print(\"from ultralytics import YOLO\")\n",
    "print(\"model = YOLO('best.pt')\")\n",
    "print(\"results = model('your_image.jpg')\")\n",
    "print(\"results[0].show()\")\n",
    "print(\"```\")\n",
    "print()\n",
    "print(\"ğŸ¥§ Ready for Raspberry Pi drone detection!\")\n",
    "print(\"ğŸŒŸ Built with Google Colab + YOLOv8 + DroneBlocks DEXI\")\n",
    "print()\n",
    "print(\"ğŸŠ\" + \"=\"*58 + \"ğŸŠ\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygspec_version": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}