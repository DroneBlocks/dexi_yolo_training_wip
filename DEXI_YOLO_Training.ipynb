{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# DEXI YOLO Training Tutorial - Local Environment\n",
    "\n",
    "This notebook walks you through the complete process of training a custom YOLO object detection model for drone detection on your local machine. We'll be working with 6 classes: bird, dog, cat, motorcycle, car, and truck.\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Dataset Exploration](#dataset-exploration) \n",
    "3. [Data Augmentation](#data-augmentation)\n",
    "4. [YOLO Training](#yolo-training)\n",
    "5. [Results Analysis](#results-analysis)\n",
    "6. [Model Testing](#model-testing)\n",
    "7. [ONNX Conversion](#onnx-conversion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-setup",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's make sure we have all the required packages installed and import the necessary libraries.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.8+\n",
    "- Virtual environment activated\n",
    "- All dependencies installed: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (only run if packages are not installed)\n",
    "# Make sure you've activated your virtual environment first!\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and check hardware acceleration\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from IPython.display import Image, display\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Set matplotlib style for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"üîß System Information:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"   MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else 'Not available'}\")\n",
    "print(f\"   MPS built: {torch.backends.mps.is_built() if hasattr(torch.backends, 'mps') else 'Not available'}\")\n",
    "\n",
    "# üöÄ Device Selection (Optimized for Apple Silicon MPS)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"\\nüöÄ Using NVIDIA GPU: {gpu_name}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(f\"\\n‚ö° Using Apple Silicon MPS acceleration!\")\n",
    "    print(f\"   This will provide 4-8x speedup over CPU\")\n",
    "    print(f\"   Expected training time: 30-45 minutes\")\n",
    "    \n",
    "    # Clear MPS cache for optimal performance\n",
    "    if torch.backends.mps.is_built():\n",
    "        torch.mps.empty_cache()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"\\nüíª Using CPU (slower but works everywhere)\")\n",
    "    print(f\"   Expected training time: 2-4 hours\")\n",
    "    print(f\"   üí° For faster training, use GPU or Apple Silicon Mac\")\n",
    "\n",
    "print(f\"\\n‚úÖ Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-exploration",
   "metadata": {},
   "source": [
    "## 2. Dataset Exploration\n",
    "\n",
    "Let's explore our dataset structure and examine the original images before augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-dataset",
   "metadata": {},
   "outputs": [],
   "source": "# Find and display our original images\noriginal_images_path = Path('train/original_image')\nimage_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n\n# Find all image files\nall_images = []\nfor ext in image_extensions:\n    all_images.extend(original_images_path.glob(ext))\n    all_images.extend(original_images_path.glob(ext.upper()))\n\nprint(f\"üì∏ Found {len(all_images)} images in the dataset\")\n\n# Display original images\nif all_images:\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    fig.suptitle('Original Images', fontsize=16, fontweight='bold')\n    \n    for idx, img_path in enumerate(all_images[:6]):\n        if idx >= 6:\n            break\n        \n        row = idx // 3\n        col = idx % 3\n        \n        # Load and display image\n        img = cv2.imread(str(img_path))\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        axes[row, col].imshow(img_rgb)\n        axes[row, col].set_title(f\"{img_path.stem}\\n{img.shape[1]}x{img.shape[0]}px\", fontweight='bold')\n        axes[row, col].axis('off')\n    \n    # Hide unused subplots\n    for idx in range(len(all_images), 6):\n        row = idx // 3\n        col = idx % 3\n        axes[row, col].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"‚ùå No images found in the train/original_image directory\")"
  },
  {
   "cell_type": "markdown",
   "id": "data-augmentation",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation\n",
    "\n",
    "Now we'll use our custom augmentation script to create multiple variations of each base image. This is crucial for training a robust YOLO model as it helps the model generalize better to different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation-overview",
   "metadata": {},
   "outputs": [],
   "source": "# Let's examine our augmentation script first\nprint(\"üîß Data Augmentation Script Overview:\")\nprint(\"\")\nprint(\"Our augmentation script applies the following transformations:\")\nprint(\"‚Ä¢ üîÑ Rotation: 0-360 degrees (random)\")\nprint(\"‚Ä¢ üìè Scaling: 0.25x to 1.3x (random)\")\nprint(\"‚Ä¢ ‚òÄÔ∏è Brightness: -30 to +30 (random)\")\nprint(\"‚Ä¢ üåà Contrast: 0.7x to 1.3x (random)\")\nprint(\"‚Ä¢ üìª Noise: Added 20% of the time\")\nprint(\"‚Ä¢ üå´Ô∏è Blur: Applied 15% of the time\")\nprint(\"\")\nprint(\"Each transformation creates realistic variations that help the model\")\nprint(\"learn to detect objects under different conditions.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation-config",
   "metadata": {},
   "outputs": [],
   "source": "# Set augmentation parameters\nAUGMENTATIONS_PER_IMAGE = 150  # Adjust this number as needed\nINPUT_DIR = \"train/original_image\"      # Directory with original images\nOUTPUT_DIR = \"train\"           # Output directory for augmented dataset\n\nprint(f\"‚öôÔ∏è Augmentation Configuration:\")\nprint(f\"Input directory: {INPUT_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Augmentations per image: {AUGMENTATIONS_PER_IMAGE}\")\nprint(f\"Expected total images: {len(all_images) * AUGMENTATIONS_PER_IMAGE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the augmentation process\n",
    "print(\"üöÄ Starting data augmentation...\")\n",
    "print(\"This may take a few minutes depending on the number of augmentations.\")\n",
    "\n",
    "# Import and use our augmentation class\n",
    "from augment_dataset import YOLODatasetAugmenter\n",
    "\n",
    "# Create augmenter instance\n",
    "augmenter = YOLODatasetAugmenter(INPUT_DIR, OUTPUT_DIR)\n",
    "\n",
    "# Run augmentation\n",
    "augmenter.augment_all_images(AUGMENTATIONS_PER_IMAGE)\n",
    "\n",
    "print(\"\\n‚úÖ Data augmentation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify augmentation results\n",
    "train_images_dir = Path('train/images')\n",
    "train_labels_dir = Path('train/labels')\n",
    "\n",
    "# Count generated files\n",
    "augmented_images = list(train_images_dir.glob('*_[0-9][0-9][0-9].jpg'))\n",
    "augmented_labels = list(train_labels_dir.glob('*_[0-9][0-9][0-9].txt'))\n",
    "original_images = [f for f in train_images_dir.glob('*.jpg') if not f.name.endswith(('_001.jpg', '_002.jpg', '_003.jpg'))]\n",
    "\n",
    "print(f\"üìä Augmentation Results:\")\n",
    "print(f\"Original images: {len(original_images)}\")\n",
    "print(f\"Augmented images: {len(augmented_images)}\")\n",
    "print(f\"Total images: {len(list(train_images_dir.glob('*.jpg')))}\")\n",
    "print(f\"Label files: {len(augmented_labels)}\")\n",
    "print(f\"\")\n",
    "print(f\"Images per class:\")\n",
    "for class_name in ['bird', 'dog', 'cat', 'motorcycle', 'car', 'truck']:\n",
    "    class_images = len(list(train_images_dir.glob(f'{class_name}_*.jpg')))\n",
    "    print(f\"  {class_name}: {class_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-augmented-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples of augmented images\n",
    "print(\"üñºÔ∏è Sample Augmented Images:\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Sample Augmented Images (Showing Transformations)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show examples from each class\n",
    "classes = ['bird', 'dog', 'cat', 'motorcycle', 'car', 'truck']\n",
    "sample_count = 0\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    class_images = list(train_images_dir.glob(f'{class_name}_*.jpg'))[:2]  # Get first 2 augmented images\n",
    "    \n",
    "    for img_idx, img_path in enumerate(class_images):\n",
    "        if sample_count >= 12:  # 3x4 grid\n",
    "            break\n",
    "            \n",
    "        row = sample_count // 4\n",
    "        col = sample_count % 4\n",
    "        \n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[row, col].imshow(img_rgb)\n",
    "        axes[row, col].set_title(f\"{class_name.title()}\\n{img_path.name}\", fontweight='bold', fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        sample_count += 1\n",
    "    \n",
    "    if sample_count >= 12:\n",
    "        break\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(sample_count, 12):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yolo-training",
   "metadata": {},
   "source": [
    "## 4. YOLO Training\n",
    "\n",
    "Now that we have our augmented dataset ready, let's train our YOLO model. We'll use the YOLOv8 architecture, which is state-of-the-art for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'model_size': 'n',        # Options: 'n', 's', 'm', 'l', 'x' (nano to extra-large)\n",
    "    'epochs': 100,            # Number of training epochs\n",
    "    'imgsz': 640,             # Image size for training\n",
    "    'batch_size': 16,         # Batch size (adjust based on your GPU memory)\n",
    "    'device': device,         # Device determined earlier\n",
    "}\n",
    "\n",
    "print(\"üéØ Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(\"\\nüí° Model Size Guide:\")\n",
    "print(\"  ‚Ä¢ 'n' (nano): Fastest, smallest, good for mobile/edge devices\")\n",
    "print(\"  ‚Ä¢ 's' (small): Good balance of speed and accuracy\")\n",
    "print(\"  ‚Ä¢ 'm' (medium): Better accuracy, moderate speed\")\n",
    "print(\"  ‚Ä¢ 'l' (large): High accuracy, slower inference\")\n",
    "print(\"  ‚Ä¢ 'x' (extra-large): Highest accuracy, slowest inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the YOLO model\n",
    "model_name = f\"yolov8{TRAINING_CONFIG['model_size']}.pt\"\n",
    "print(f\"ü§ñ Loading {model_name} model...\")\n",
    "\n",
    "# Load pre-trained YOLO model\n",
    "model = YOLO(model_name)\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Model: YOLOv8{TRAINING_CONFIG['model_size']}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.model.parameters()):,}\")\n",
    "print(f\"   Size on disk: {os.path.getsize(model_name) / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": "# Start training with automatic train/val split\nprint(\"üöÄ Starting YOLO training...\")\nprint(\"This will take some time. You can monitor the progress below.\")\nprint(\"Training logs and checkpoints will be saved in 'runs/detect/drone_detection/'\")\nprint(\"üí° YOLO will automatically split training data for validation (80% train, 20% val)\")\n\n# Train the model using the parameters from train_yolo.py\nresults = model.train(\n    data='dataset.yaml',\n    epochs=TRAINING_CONFIG['epochs'],\n    imgsz=TRAINING_CONFIG['imgsz'],\n    batch=TRAINING_CONFIG['batch_size'],\n    device=TRAINING_CONFIG['device'],\n    project='runs/detect',\n    name='drone_detection',\n    save_period=10,  # Save checkpoint every 10 epochs\n    patience=20,     # Early stopping patience\n    \n    # Automatic train/validation split using fraction parameter\n    fraction=0.8,    # Use 80% of data for training (20% for validation)\n    \n    # Augmentation settings (additional to our pre-generated augmentations)\n    hsv_h=0.015,     # Hue augmentation\n    hsv_s=0.7,       # Saturation augmentation  \n    hsv_v=0.4,       # Value augmentation\n    degrees=0,       # Don't add rotation (we already did this)\n    translate=0.1,   # Translation augmentation\n    scale=0.1,       # Additional scale augmentation\n    shear=0.1,       # Shear augmentation\n    perspective=0.0, # Perspective augmentation\n    flipud=0.0,      # No vertical flip (objects have orientation)\n    fliplr=0.0,      # No horizontal flip (for consistency)\n    mosaic=0.8,      # Mosaic augmentation probability\n    mixup=0.1,       # Mixup augmentation probability\n    \n    # Optimization\n    optimizer='AdamW',\n    lr0=0.01,        # Initial learning rate\n    lrf=0.1,         # Final learning rate (lr0 * lrf)\n    momentum=0.937,\n    weight_decay=0.0005,\n    warmup_epochs=3,\n    warmup_momentum=0.8,\n    warmup_bias_lr=0.1,\n    \n    # Other settings\n    box=7.5,         # Box loss gain\n    cls=0.5,         # Class loss gain\n    dfl=1.5,         # DFL loss gain\n    verbose=True,\n)\n\nprint(\"\\nüéâ Training completed!\")"
  },
  {
   "cell_type": "markdown",
   "id": "results-analysis",
   "metadata": {},
   "source": [
    "## 5. Results Analysis\n",
    "\n",
    "Let's analyze the training results and visualize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training results exist\n",
    "results_dir = Path('runs/detect/drone_detection4')\n",
    "if results_dir.exists():\n",
    "    print(f\"üìÇ Training results saved in: {results_dir}\")\n",
    "    print(f\"üìÅ Contents:\")\n",
    "    for item in sorted(results_dir.iterdir()):\n",
    "        if item.is_file():\n",
    "            print(f\"  üìÑ {item.name}\")\n",
    "        else:\n",
    "            print(f\"  üìÅ {item.name}/\")\n",
    "else:\n",
    "    print(\"‚ùå Training results not found. Make sure training completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-training-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training curves if available\n",
    "results_image_path = results_dir / 'results.png'\n",
    "if results_image_path.exists():\n",
    "    print(\"üìà Training Results:\")\n",
    "    display(Image(str(results_image_path)))\n",
    "else:\n",
    "    print(\"üìà Training curves not found. They should be available after training completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix if available\n",
    "confusion_matrix_path = results_dir / 'confusion_matrix.png'\n",
    "if confusion_matrix_path.exists():\n",
    "    print(\"üéØ Confusion Matrix:\")\n",
    "    display(Image(str(confusion_matrix_path)))\n",
    "else:\n",
    "    print(\"üéØ Confusion matrix not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display validation results\n",
    "val_batch_path = results_dir / 'val_batch0_labels.jpg'\n",
    "if val_batch_path.exists():\n",
    "    print(\"üîç Validation Batch with Ground Truth Labels:\")\n",
    "    display(Image(str(val_batch_path)))\n",
    "    \n",
    "val_pred_path = results_dir / 'val_batch0_pred.jpg'\n",
    "if val_pred_path.exists():\n",
    "    print(\"\\nü§ñ Validation Batch with Model Predictions:\")\n",
    "    display(Image(str(val_pred_path)))\n",
    "\n",
    "if not val_batch_path.exists() and not val_pred_path.exists():\n",
    "    print(\"üîç Validation images not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and run validation\n",
    "best_model_path = results_dir / 'weights' / 'best.pt'\n",
    "if best_model_path.exists():\n",
    "    print(f\"üèÜ Loading best model: {best_model_path}\")\n",
    "    trained_model = YOLO(str(best_model_path))\n",
    "    \n",
    "    # Run validation\n",
    "    print(\"\\nüî¨ Running final validation...\")\n",
    "    val_results = trained_model.val()\n",
    "    \n",
    "    # Print key metrics\n",
    "    print(\"\\nüìä Final Model Metrics:\")\n",
    "    if hasattr(val_results, 'box'):\n",
    "        metrics = val_results.box\n",
    "        print(f\"  mAP@0.5: {metrics.map50:.3f}\")\n",
    "        print(f\"  mAP@0.5:0.95: {metrics.map:.3f}\")\n",
    "        print(f\"  Precision: {metrics.mp:.3f}\")\n",
    "        print(f\"  Recall: {metrics.mr:.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå Best model not found. Training may not have completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-testing",
   "metadata": {},
   "source": [
    "## 6. Model Testing\n",
    "\n",
    "Let's test our trained model on some sample images to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on some training images\n",
    "if 'trained_model' in locals():\n",
    "    print(\"üß™ Testing the trained model on sample images...\")\n",
    "    \n",
    "    # Get some test images\n",
    "    test_images = list(train_images_dir.glob('*_001.jpg'))[:6]  # First augmented image of each class\n",
    "    \n",
    "    if test_images:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Model Predictions on Sample Images', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for idx, img_path in enumerate(test_images[:6]):\n",
    "            row = idx // 3\n",
    "            col = idx % 3\n",
    "            \n",
    "            # Run inference\n",
    "            results = trained_model(str(img_path), verbose=False)\n",
    "            \n",
    "            # Get the annotated image\n",
    "            annotated_img = results[0].plot()\n",
    "            annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[row, col].imshow(annotated_img_rgb)\n",
    "            axes[row, col].set_title(f\"{img_path.stem}\", fontweight='bold')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ùå No test images found.\")\n",
    "else:\n",
    "    print(\"‚ùå Trained model not available. Please complete the training step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-test-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test model on a custom image\n",
    "def test_on_custom_image(image_path, confidence_threshold=0.5):\n",
    "    \"\"\"Test the model on a custom image\"\"\"\n",
    "    if 'trained_model' not in locals():\n",
    "        print(\"‚ùå Trained model not available. Please complete the training step first.\")\n",
    "        return\n",
    "    \n",
    "    if not Path(image_path).exists():\n",
    "        print(f\"‚ùå Image not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Testing on: {image_path}\")\n",
    "    \n",
    "    # Run inference\n",
    "    results = trained_model(image_path, conf=confidence_threshold, verbose=False)\n",
    "    \n",
    "    # Display results\n",
    "    annotated_img = results[0].plot()\n",
    "    annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(annotated_img_rgb)\n",
    "    plt.title(f'Detection Results - {Path(image_path).name}', fontweight='bold', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load dataset config to get class names\n",
    "    with open('dataset.yaml', 'r') as f:\n",
    "        dataset_config = yaml.safe_load(f)\n",
    "    \n",
    "    # Print detection details\n",
    "    if len(results[0].boxes) > 0:\n",
    "        print(\"\\nüéØ Detections:\")\n",
    "        for i, box in enumerate(results[0].boxes):\n",
    "            class_id = int(box.cls[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            class_name = dataset_config['names'][class_id]\n",
    "            print(f\"  {i+1}. {class_name} (confidence: {confidence:.3f})\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No objects detected\")\n",
    "\n",
    "# Example usage (uncomment and modify the path to test on your own images)\n",
    "# test_on_custom_image('path/to/your/test/image.jpg', confidence_threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "onnx-conversion",
   "metadata": {},
   "source": [
    "## 7. ONNX Conversion for Deployment\n",
    "\n",
    "Now let's convert our trained PyTorch model to ONNX format for efficient deployment on devices like Raspberry Pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to ONNX format optimized for Pi deployment\n",
    "from convert_to_onnx import convert_model_to_onnx\n",
    "\n",
    "if 'best_model_path' in locals() and best_model_path.exists():\n",
    "    print(\"üöÄ Converting trained model to ONNX...\")\n",
    "    \n",
    "    # Convert with 320x320 input size (optimal for Pi camera 320x240)\n",
    "    onnx_path = convert_model_to_onnx(\n",
    "        model_path=str(best_model_path),\n",
    "        imgsz=320,  # Optimized for Pi camera\n",
    "        half=False,  # Keep FP32 for better Pi compatibility\n",
    "        simplify=True\n",
    "    )\n",
    "    \n",
    "    if onnx_path:\n",
    "        print(f\"\\n‚úÖ ONNX conversion successful!\")\n",
    "        print(f\"üìÅ ONNX model saved: {onnx_path}\")\n",
    "        print(f\"ü•ß Ready for Raspberry Pi deployment!\")\n",
    "        \n",
    "        # Test the ONNX model\n",
    "        try:\n",
    "            import onnxruntime as ort\n",
    "            session = ort.InferenceSession(onnx_path)\n",
    "            print(f\"\\nüîç ONNX Model Info:\")\n",
    "            print(f\"   Input shape: {session.get_inputs()[0].shape}\")\n",
    "            print(f\"   Output shape: {session.get_outputs()[0].shape}\")\n",
    "            print(f\"   Providers: {ort.get_available_providers()}\")\n",
    "        except ImportError:\n",
    "            print(f\"\\n‚ö†Ô∏è  Install onnxruntime to test: pip install onnxruntime\")\n",
    "    else:\n",
    "        print(\"‚ùå ONNX conversion failed\")\n",
    "else:\n",
    "    print(\"‚ùå No trained model found. Please complete training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed the YOLO training tutorial! Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ What You've Done:\n",
    "1. **Environment Setup**: Configured all required libraries and dependencies\n",
    "2. **Dataset Exploration**: Examined the original dataset structure and images\n",
    "3. **Data Augmentation**: Generated hundreds of augmented training images with various transformations\n",
    "4. **YOLO Training**: Trained a custom YOLOv8 model on your 6-class dataset\n",
    "5. **Results Analysis**: Evaluated model performance with metrics and visualizations\n",
    "6. **Model Testing**: Tested the trained model on sample images\n",
    "7. **ONNX Conversion**: Converted the model for Pi deployment\n",
    "\n",
    "### üìÅ Important Files Created:\n",
    "- `runs/detect/drone_detection/weights/best.pt` - Your best trained model\n",
    "- `runs/detect/drone_detection/weights/last.pt` - Last checkpoint\n",
    "- `runs/detect/drone_detection/results.png` - Training curves\n",
    "- `train/images/` - Augmented training images\n",
    "- `train/labels/` - Corresponding YOLO format labels\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Export Your Model**: Convert to different formats (ONNX, TensorRT, etc.) for deployment\n",
    "2. **Create Validation Set**: Prepare a separate validation dataset for final testing\n",
    "3. **Optimize for Deployment**: Experiment with different model sizes and quantization\n",
    "4. **Real-world Testing**: Test on actual drone footage or real-world scenarios\n",
    "\n",
    "### üìö Additional Resources:\n",
    "- [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [YOLO Model Export Guide](https://docs.ultralytics.com/modes/export/)\n",
    "- [Advanced Training Techniques](https://docs.ultralytics.com/modes/train/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy detecting! üéØü§ñ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéä TUTORIAL COMPLETED SUCCESSFULLY! üéä\")\n",
    "print(\"\")\n",
    "print(\"Your YOLO model is now trained and ready to use!\")\n",
    "print(f\"Best model saved at: runs/detect/drone_detection/weights/best.pt\")\n",
    "print(\"\")\n",
    "print(\"To use your model in a Python script:\")\n",
    "print(\"\")\n",
    "print(\"```python\")\n",
    "print(\"from ultralytics import YOLO\")\n",
    "print(\"\")\n",
    "print(\"# Load your trained model\")\n",
    "print(\"model = YOLO('runs/detect/drone_detection/weights/best.pt')\")\n",
    "print(\"\")\n",
    "print(\"# Run inference on an image\")\n",
    "print(\"results = model('path/to/your/image.jpg')\")\n",
    "print(\"\")\n",
    "print(\"# Show results\")\n",
    "print(\"results[0].show()\")\n",
    "print(\"```\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}